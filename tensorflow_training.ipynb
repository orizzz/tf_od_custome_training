{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install tensorflow --ignore-installed --upgrade \n",
    "#tensorflow-gpu==2.4.0 \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 471 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 744 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.22.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 583 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting numpy>=1.14.5\n",
      "  Downloading numpy-1.21.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 442 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel<1.0,>=0.32.0\n",
      "  Using cached wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 376 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.13.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 869 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Using cached libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4 MB)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-59.2.0-py3-none-any.whl (952 kB)\n",
      "\u001b[K     |████████████████████████████████| 952 kB 403 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 529 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, six, setuptools, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.7 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.8.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 numpy-1.21.4 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-59.2.0 six-1.16.0 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0 typing-extensions-4.0.0 urllib3-1.26.7 werkzeug-2.0.2 wheel-0.37.0 wrapt-1.13.3 zipp-3.6.0\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GnVG7OmvsAOR",
    "outputId": "53d6d057-7af6-4b30-e43a-fba64f321ea4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "pip list"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Package                      Version\n",
      "---------------------------- ---------\n",
      "absl-py                      1.0.0\n",
      "astunparse                   1.6.3\n",
      "backcall                     0.2.0\n",
      "cachetools                   4.2.4\n",
      "certifi                      2021.10.8\n",
      "charset-normalizer           2.0.7\n",
      "debugpy                      1.5.1\n",
      "decorator                    5.1.0\n",
      "entrypoints                  0.3\n",
      "flatbuffers                  2.0\n",
      "gast                         0.4.0\n",
      "google-auth                  2.3.3\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.42.0\n",
      "h5py                         3.6.0\n",
      "idna                         3.3\n",
      "importlib-metadata           4.8.2\n",
      "ipykernel                    6.5.1\n",
      "ipython                      7.29.0\n",
      "jedi                         0.18.1\n",
      "jupyter-client               7.1.0\n",
      "jupyter-core                 4.9.1\n",
      "keras                        2.7.0\n",
      "Keras-Preprocessing          1.1.2\n",
      "libclang                     12.0.0\n",
      "Markdown                     3.3.6\n",
      "matplotlib-inline            0.1.3\n",
      "nest-asyncio                 1.5.1\n",
      "numpy                        1.21.4\n",
      "oauthlib                     3.1.1\n",
      "opt-einsum                   3.3.0\n",
      "parso                        0.8.2\n",
      "pexpect                      4.8.0\n",
      "pickleshare                  0.7.5\n",
      "pip                          20.3.4\n",
      "pkg-resources                0.0.0\n",
      "prompt-toolkit               3.0.22\n",
      "protobuf                     3.19.1\n",
      "ptyprocess                   0.7.0\n",
      "pyasn1                       0.4.8\n",
      "pyasn1-modules               0.2.8\n",
      "Pygments                     2.10.0\n",
      "python-dateutil              2.8.2\n",
      "pyzmq                        22.3.0\n",
      "requests                     2.26.0\n",
      "requests-oauthlib            1.3.0\n",
      "rsa                          4.7.2\n",
      "setuptools                   59.2.0\n",
      "six                          1.16.0\n",
      "tensorboard                  2.7.0\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.0\n",
      "tensorflow                   2.7.0\n",
      "tensorflow-estimator         2.7.0\n",
      "tensorflow-io-gcs-filesystem 0.22.0\n",
      "termcolor                    1.1.0\n",
      "tornado                      6.1\n",
      "traitlets                    5.1.1\n",
      "typing-extensions            4.0.0\n",
      "urllib3                      1.26.7\n",
      "wcwidth                      0.2.5\n",
      "Werkzeug                     2.0.2\n",
      "wheel                        0.37.0\n",
      "wrapt                        1.13.3\n",
      "zipp                         3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import tensorflow as tf\n",
    "print(\"tensorflow_version :\",tf.__version__)\n",
    "\n",
    "sys_details = tf.sysconfig.get_build_info()\n",
    "cuda_version = sys_details[\"cuda_version\"]\n",
    "print(\"cuda_version :\",cuda_version)\n",
    "  \n",
    "cudnn_version = sys_details[\"cudnn_version\"]  \n",
    "print(\"cudnn_version :\", cudnn_version)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensorflow_version : 2.7.0\n",
      "cuda_version : 11.2\n",
      "cudnn_version : 8\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeyP0aYes9Zs",
    "outputId": "446538fc-01d0-4b95-c156-0347649feb24"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Sun_Feb_14_21:12:58_PST_2021\n",
      "Cuda compilation tools, release 11.2, V11.2.152\n",
      "Build cuda_11.2.r11.2/compiler.29618528_0\n",
      "Failed to initialize NVML: Driver/library version mismatch\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8AOOLDGAsPW",
    "outputId": "0e957e32-a99b-4bba-80a9-9490005ef83e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Cloning TFOD 2.0 Github**"
   ],
   "metadata": {
    "id": "DEGpXLl3tQVX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "!git clone https://github.com/tensorflow/models.git"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 67412, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 67412 (delta 0), reused 3 (delta 0), pack-reused 67405\u001b[K\n",
      "Receiving objects: 100% (67412/67412), 576.22 MiB | 459.00 KiB/s, done.\n",
      "Resolving deltas: 100% (47310/47310), done.\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFkdXoEltLY9",
    "outputId": "7eb1d02e-d54f-4ebc-9ad7-da18d9cf6fbf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aJ4YzpQ4tMlz",
    "outputId": "e0f424d7-938e-4707-c905-7d3d17b7aec5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "cd models/research"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZwI0sTdtMsc",
    "outputId": "991074c3-5878-41d2-bba0-98c65e07cdb7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "SK5RyNz9tMvZ",
    "outputId": "436d4afe-bdfc-4c26-b61d-552adb5db5fb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "!protoc object_detection/protos/*.proto --python_out=."
   ],
   "outputs": [],
   "metadata": {
    "id": "NozH3MfAtMyR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "!git clone https://github.com/cocodataset/cocoapi.git\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'cocoapi'...\n",
      "remote: Enumerating objects: 975, done.\u001b[K\n",
      "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
      "Receiving objects: 100% (975/975), 11.72 MiB | 458.00 KiB/s, done.\n",
      "Resolving deltas: 100% (576/576), done.\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alOqL7ortM1F",
    "outputId": "c872534d-2d0d-4bb4-fbdf-98d009ea6699"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "cd cocoapi/PythonAPI"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research/cocoapi/PythonAPI\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XRlMiuEtM4R",
    "outputId": "07c01d9b-d900-46de-9cb5-a5d7f51fe803"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting cython\n",
      "  Using cached Cython-0.29.24-cp39-cp39-manylinux1_x86_64.whl (1.9 MB)\n",
      "Installing collected packages: cython\n",
      "Successfully installed cython-0.29.24\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "!make"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "python setup.py build_ext --inplace\n",
      "running build_ext\n",
      "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "building 'pycocotools._mask' extension\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -ffile-prefix-map=/build/python3.9-1n6GrT/python3.9-3.9.5=. -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -ffile-prefix-map=/build/python3.9-1n6GrT/python3.9-3.9.5=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages/numpy/core/include -I../common -I/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/include -I/usr/include/python3.9 -c ../common/maskApi.c -o build/temp.linux-x86_64-3.9/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "   46 |       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "      |       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   46 |       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
      "      |                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  166 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "  166 |   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
      "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  167 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "  167 |   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
      "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  212 |       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "      |       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "  212 |       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
      "      |                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  220 |   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
      "  220 |   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
      "      |                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  228 |     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "      |     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "  228 |     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
      "      |                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -ffile-prefix-map=/build/python3.9-1n6GrT/python3.9-3.9.5=. -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -ffile-prefix-map=/build/python3.9-1n6GrT/python3.9-3.9.5=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages/numpy/core/include -I../common -I/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/include -I/usr/include/python3.9 -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.9/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "creating build/lib.linux-x86_64-3.9\n",
      "creating build/lib.linux-x86_64-3.9/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -ffile-prefix-map=/build/python3.9-1n6GrT/python3.9-3.9.5=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.9/../common/maskApi.o build/temp.linux-x86_64-3.9/pycocotools/_mask.o -o build/lib.linux-x86_64-3.9/pycocotools/_mask.cpython-39-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-3.9/pycocotools/_mask.cpython-39-x86_64-linux-gnu.so -> pycocotools\n",
      "rm -rf build\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZ-xj6MUtM7U",
    "outputId": "daad3ed7-9a5e-4d54-9299-5d324fb1d954"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "cp -r pycocotools /models/research/"
   ],
   "outputs": [],
   "metadata": {
    "id": "A5Qr7ACCtM_b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Makefile  pycocoDemo.ipynb  pycocoEvalDemo.ipynb  \u001b[0m\u001b[01;34mpycocotools\u001b[0m/  setup.py\r\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install the Object Detection API"
   ],
   "metadata": {
    "id": "rLq79dR0uQFt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "cd .."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research/cocoapi\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM2bgHvLtNFt",
    "outputId": "e23a3979-cb0f-4a8a-c834-e69f38dacaf2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "cd .. "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZouxA5TuWgV",
    "outputId": "61f32833-47f3-468c-b299-eb0c86faace9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wvCptm6TDmT8",
    "outputId": "ba534e2c-2454-41f6-b8f8-8b56e8c2c384"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "cp object_detection/packages/tf2/setup.py ."
   ],
   "outputs": [],
   "metadata": {
    "id": "Q635Jl58uWjI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "pip install --upgrade pip"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pip in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (20.3.4)\n",
      "Collecting pip\n",
      "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 468 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.3.4\n",
      "    Uninstalling pip-20.3.4:\n",
      "      Successfully uninstalled pip-20.3.4\n",
      "Successfully installed pip-21.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUna9O2e88B_",
    "outputId": "1b85a7a2-b71a-438a-babc-2c4c61a0ed41"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "!python -m pip install --use-feature=2020-resolver ."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
      "Processing /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting avro-python3\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting apache-beam\n",
      "  Downloading apache-beam-2.34.0.zip (2.6 MB)\n",
      "     |████████████████████████████████| 2.6 MB 469 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading Pillow-8.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 459 kB/s            \n",
      "\u001b[?25hCollecting lxml\n",
      "  Downloading lxml-4.6.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "     |████████████████████████████████| 6.9 MB 429 kB/s            \n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "     |████████████████████████████████| 11.2 MB 108 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: Cython in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from object-detection==0.1) (0.29.24)\n",
      "Collecting contextlib2\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting tf-slim\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "     |████████████████████████████████| 352 kB 448 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: six in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from object-detection==0.1) (1.16.0)\n",
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.2-cp39-cp39-linux_x86_64.whl\n",
      "Collecting lvis\n",
      "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "     |████████████████████████████████| 39.8 MB 529 kB/s            \n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.3.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "     |████████████████████████████████| 11.5 MB 403 kB/s            \n",
      "\u001b[?25hCollecting tf-models-official>=2.5.1\n",
      "  Downloading tf_models_official-2.7.0-py2.py3-none-any.whl (1.8 MB)\n",
      "     |████████████████████████████████| 1.8 MB 482 kB/s            \n",
      "\u001b[?25hCollecting tensorflow_io\n",
      "  Downloading tensorflow_io-0.22.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
      "     |████████████████████████████████| 22.7 MB 469 kB/s            \n",
      "\u001b[?25hCollecting keras==2.6.0\n",
      "  Using cached keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "     |████████████████████████████████| 61 kB 852 kB/s            \n",
      "\u001b[?25hCollecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.4.60-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
      "     |████████████████████████████████| 47.6 MB 502 kB/s            \n",
      "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
      "     |████████████████████████████████| 213 kB 714 kB/s            \n",
      "\u001b[?25hCollecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     |████████████████████████████████| 43 kB 588 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-api-python-client>=1.6.7\n",
      "  Downloading google_api_python_client-2.31.0-py2.py3-none-any.whl (7.8 MB)\n",
      "     |████████████████████████████████| 7.8 MB 502 kB/s            \n",
      "\u001b[?25hCollecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "     |████████████████████████████████| 58 kB 953 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tensorflow>=2.7.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.15.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 404 kB/s            \n",
      "\u001b[?25hCollecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "     |████████████████████████████████| 90 kB 987 kB/s            \n",
      "\u001b[?25hCollecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "     |████████████████████████████████| 98 kB 943 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.21.4)\n",
      "Collecting tensorflow-hub>=0.6.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "     |████████████████████████████████| 108 kB 803 kB/s            \n",
      "\u001b[?25hCollecting tensorflow-text>=2.7.0\n",
      "  Downloading tensorflow_text-2.7.3-cp39-cp39-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 416 kB/s            \n",
      "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "     |████████████████████████████████| 99 kB 910 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 477 kB/s            \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 473 kB/s            \n",
      "\u001b[?25hCollecting psutil>=5.4.3\n",
      "  Downloading psutil-5.8.0-cp39-cp39-manylinux2010_x86_64.whl (293 kB)\n",
      "     |████████████████████████████████| 293 kB 534 kB/s            \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "     |████████████████████████████████| 661 kB 470 kB/s            \n",
      "\u001b[?25hCollecting pytz>=2017.3\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "     |████████████████████████████████| 503 kB 727 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from pandas->object-detection==0.1) (2.8.2)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tf-slim->object-detection==0.1) (1.0.0)\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "     |████████████████████████████████| 89 kB 755 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "     |████████████████████████████████| 151 kB 822 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fastavro<2,>=0.21.4\n",
      "  Downloading fastavro-1.4.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "     |████████████████████████████████| 2.5 MB 585 kB/s            \n",
      "\u001b[?25hCollecting future<1.0.0,>=0.18.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "     |████████████████████████████████| 829 kB 446 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.29.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from apache-beam->object-detection==0.1) (1.42.0)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting httplib2<0.20.0,>=0.8\n",
      "  Downloading httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 921 kB/s            \n",
      "\u001b[?25hCollecting numpy>=1.15.4\n",
      "  Downloading numpy-1.20.3-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.4 MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     |████████████████████████████████| 15.4 MB 438 kB/s            \n",
      "\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (516 kB)\n",
      "     |████████████████████████████████| 516 kB 534 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12.2 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from apache-beam->object-detection==0.1) (3.19.1)\n",
      "Collecting pyarrow<6.0.0,>=0.15.1\n",
      "  Downloading pyarrow-5.0.0-cp39-cp39-manylinux2014_x86_64.whl (23.7 MB)\n",
      "     |████████████████████████████████| 23.7 MB 581 kB/s            \n",
      "\u001b[?25hCollecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from apache-beam->object-detection==0.1) (2.26.0)\n",
      "Collecting typing-extensions<4,>=3.7.0\n",
      "  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.6.4-cp39-cp39-manylinux_2_24_x86_64.whl (250 kB)\n",
      "     |████████████████████████████████| 250 kB 809 kB/s            \n",
      "\u001b[?25hCollecting opencv-python>=4.1.0.25\n",
      "  Downloading opencv_python-4.5.4.60-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
      "     |████████████████████████████████| 60.3 MB 427 kB/s            \n",
      "\u001b[?25hCollecting kiwisolver>=1.1.0\n",
      "  Downloading kiwisolver-1.3.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "     |████████████████████████████████| 1.6 MB 572 kB/s            \n",
      "\u001b[?25hCollecting pyparsing>=2.4.0\n",
      "  Using cached pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
      "Collecting cycler>=0.10.0\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting setuptools-scm>=4\n",
      "  Using cached setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.2-py3-none-any.whl (880 kB)\n",
      "     |████████████████████████████████| 880 kB 361 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=18.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from pycocotools->object-detection==0.1) (59.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.22.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorflow_io->object-detection==0.1) (0.22.0)\n",
      "Collecting uritemplate<5,>=3.0.0\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=1.21.0\n",
      "  Downloading google_api_core-2.2.2-py2.py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 895 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.3.3)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyparsing>=2.4.0\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 1.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.10.8)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Requirement already satisfied: urllib3 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.26.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.3)\n",
      "Collecting tomli>=1.0.0\n",
      "  Using cached tomli-1.2.2-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
      "INFO: pip is looking at multiple versions of setuptools-scm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting setuptools-scm>=4\n",
      "  Downloading setuptools_scm-6.3.1-py3-none-any.whl (33 kB)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "     |████████████████████████████████| 630 kB 418 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "     |████████████████████████████████| 510 kB 612 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of python-dateutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "INFO: pip is looking at multiple versions of pyparsing to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyparsing>=2.4.0\n",
      "  Downloading pyparsing-2.4.6-py2.py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 814 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pymongo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (531 kB)\n",
      "     |████████████████████████████████| 531 kB 872 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pydot to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "INFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyarrow<6.0.0,>=0.15.1\n",
      "  Downloading pyarrow-4.0.1-cp39-cp39-manylinux2014_x86_64.whl (21.9 MB)\n",
      "     |████████████████████████████████| 21.9 MB 504 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of py-cpuinfo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
      "     |████████████████████████████████| 95 kB 657 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of psutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting psutil>=5.4.3\n",
      "  Downloading psutil-5.7.3.tar.gz (465 kB)\n",
      "     |████████████████████████████████| 465 kB 733 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf<4,>=3.12.2\n",
      "  Using cached protobuf-3.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "INFO: pip is looking at multiple versions of packaging to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.2-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 767 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of orjson to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.6.3-cp39-cp39-manylinux_2_24_x86_64.whl (234 kB)\n",
      "     |████████████████████████████████| 234 kB 548 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python>=4.1.0.25\n",
      "  Using cached opencv_python-4.5.4.58-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
      "INFO: pip is looking at multiple versions of oauth2client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.2-py2.py3-none-any.whl (99 kB)\n",
      "     |████████████████████████████████| 99 kB 775 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy>=1.15.4\n",
      "  Downloading numpy-1.20.2-cp39-cp39-manylinux2010_x86_64.whl (15.4 MB)\n",
      "     |████████████████████████████████| 15.4 MB 427 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of kiwisolver to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kiwisolver>=1.1.0\n",
      "  Downloading kiwisolver-1.3.1-cp39-cp39-manylinux1_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 501 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of kaggle to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.10.tar.gz (59 kB)\n",
      "     |████████████████████████████████| 59 kB 800 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of httplib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httplib2<0.20.0,>=0.8\n",
      "  Downloading httplib2-0.19.0-py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 1.0 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of hdfs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.5.8.tar.gz (41 kB)\n",
      "     |████████████████████████████████| 41 kB 529 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio<2,>=1.29.0\n",
      "  Using cached grpcio-1.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "INFO: pip is looking at multiple versions of google-api-python-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Downloading google_api_python_client-2.30.0-py2.py3-none-any.whl (7.8 MB)\n",
      "     |████████████████████████████████| 7.8 MB 546 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of future to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of fonttools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.1-py3-none-any.whl (873 kB)\n",
      "     |████████████████████████████████| 873 kB 471 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of fastavro to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Downloading fastavro-1.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "     |████████████████████████████████| 2.5 MB 431 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of dill to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of cycler to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cycler>=0.10.0\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "INFO: pip is looking at multiple versions of crcmod to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting absl-py>=0.2.2\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-io-gcs-filesystem to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-io-gcs-filesystem==0.22.0\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.22.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-io to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow_io\n",
      "  Downloading tensorflow_io-0.21.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
      "     |████████████████████████████████| 22.7 MB 495 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pycocotools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.1.tar.gz (23 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of lxml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.3-cp39-cp39-manylinux2014_x86_64.whl (6.9 MB)\n",
      "     |████████████████████████████████| 6.9 MB 442 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pillow\n",
      "  Downloading Pillow-8.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "     |████████████████████████████████| 3.0 MB 453 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.4.3-cp39-cp39-manylinux1_x86_64.whl (10.3 MB)\n",
      "INFO: pip is looking at multiple versions of lvis to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lvis\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Downloading lvis-0.5.2-py3-none-any.whl (14 kB)\n",
      "INFO: pip is looking at multiple versions of cython to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting Cython\n",
      "  Using cached Cython-0.29.24-cp39-cp39-manylinux1_x86_64.whl (1.9 MB)\n",
      "INFO: pip is looking at multiple versions of contextlib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting contextlib2\n",
      "  Downloading contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)\n",
      "INFO: pip is looking at multiple versions of avro-python3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.9.1.tar.gz (36 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of apache-beam to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting apache-beam\n",
      "  Downloading apache-beam-2.33.0.zip (2.6 MB)\n",
      "     |████████████████████████████████| 2.6 MB 502 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tf-slim to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
      "     |████████████████████████████████| 28.5 MB 425 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "     |████████████████████████████████| 11.5 MB 506 kB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tf-models-official to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tf-models-official>=2.5.1\n",
      "  Downloading tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
      "     |████████████████████████████████| 1.8 MB 503 kB/s            \n",
      "\u001b[?25hCollecting tensorflow>=2.5.0\n",
      "  Downloading tensorflow-2.6.2-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\n",
      "     |████████████████████████████████| 458.4 MB 68 kB/s             \n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Using cached absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "Collecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Collecting six\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorboard<2.7,>=2.6.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "     |████████████████████████████████| 5.6 MB 389 kB/s            \n",
      "\u001b[?25hCollecting numpy>=1.15.4\n",
      "  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n",
      "     |████████████████████████████████| 14.9 MB 590 kB/s            \n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Using cached clang-5.0-py3-none-any.whl\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1-cp39-cp39-linux_x86_64.whl\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.21.0\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.21.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Downloading dm_tree-0.1.6-cp39-cp39-manylinux_2_24_x86_64.whl (94 kB)\n",
      "     |████████████████████████████████| 94 kB 359 kB/s             \n",
      "\u001b[?25hCollecting tensorflow-text>=2.5.0\n",
      "  Downloading tensorflow_text-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 428 kB/s            \n",
      "\u001b[?25h  Downloading tensorflow_text-2.6.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "     |████████████████████████████████| 4.4 MB 483 kB/s            \n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.11.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n",
      "     |████████████████████████████████| 763 kB 431 kB/s            \n",
      "\u001b[?25hCollecting scikit-learn>=0.21.3\n",
      "  Downloading scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "     |████████████████████████████████| 24.7 MB 466 kB/s            \n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.2-py3-none-any.whl (17 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.4.0-py3-none-any.whl (48 kB)\n",
      "     |████████████████████████████████| 48 kB 705 kB/s            \n",
      "\u001b[?25hCollecting attrs>=18.1.0\n",
      "  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
      "     |████████████████████████████████| 53 kB 809 kB/s            \n",
      "\u001b[?25hCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "     |████████████████████████████████| 198 kB 607 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "     |████████████████████████████████| 306 kB 738 kB/s            \n",
      "\u001b[?25hCollecting google-auth<3.0.0dev,>=1.16.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |████████████████████████████████| 152 kB 584 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.6)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "     |████████████████████████████████| 78 kB 550 kB/s            \n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "     |████████████████████████████████| 129 kB 757 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
      "Building wheels for collected packages: object-detection, apache-beam, avro-python3, crcmod, dill, future, kaggle, py-cpuinfo, seqeval, docopt, promise\n",
      "  Building wheel for object-detection (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1681358 sha256=959d8d77c66a7466b911c3ef7f5eadd6793fc49a5133e79e5a24c7ff1d698ac1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ecbfkyvv/wheels/e2/7d/6d/f71beae8a75f20958b1831895bdf6932b4d095600a8d4689da\n",
      "  Building wheel for apache-beam (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for apache-beam: filename=apache_beam-2.34.0-cp39-cp39-linux_x86_64.whl size=10515524 sha256=77611ab7c898797800c3a9ecd52233f71dfa1d571d4f5fe71e7db397b1c18ab8\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/c9/d5/45/b84ce561afaafb78631657591196400b8e99453a902e78ef70\n",
      "  Building wheel for avro-python3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=ffa05ea2f144538e3278856fb504e75fbfc19e596cb0db845624107b632f043f\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/ed/92/35/2bdc9f2ca88309e91f2374b17c69836484a7566f34d5612613\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp39-cp39-linux_x86_64.whl size=31335 sha256=4ee77e5799da3033044605c800443cdce3492a03558e0920dc7af74780df4a5e\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=ef6d67a47738e68f12eb6a12701aa2319b4bf2e18d4efd7444b9b8032e608ba3\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/4f/0b/ce/75d96dd714b15e51cb66db631183ea3844e0c4a6d19741a149\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=654eee2cfd40900b13b56a177d6f61713425fa0f90a968424286ae075aa538ce\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/2f/a0/d3/4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=3eae8b0d31f19b06818ce4761cca053a1236ebca249d3fb00dc0b5a9f1755031\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/ac/b2/c3/fa4706d469b5879105991d1c8be9a3c2ef329ba9fe2ce5085e\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=4e73d38cdd0c6663d4663157188dda5a885c70219280bef139724146c4b5e272\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/a9/33/c2/bcf6550ff9c95f699d7b2f261c8520b42b7f7c33b6e6920e29\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=549fbbc4285455b759ae06180121ec7692dbee1a84c4c615af8ed8394022690d\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=ebbe02215e7580735a3c42de4cd23e86c0500938f9d793c1a68024d0b7d0c009\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=448ca314271462bf75249a5250aa71b3652aea142936ecaf7495bff2bdfc1be6\n",
      "  Stored in directory: /home/oriz_1st/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "Successfully built object-detection apache-beam avro-python3 crcmod dill future kaggle py-cpuinfo seqeval docopt promise\n",
      "Installing collected packages: six, pyparsing, google-auth, tomli, packaging, numpy, absl-py, wrapt, typing-extensions, threadpoolctl, text-unidecode, tensorflow-estimator, tensorboard, setuptools-scm, scipy, pillow, kiwisolver, keras, joblib, httplib2, h5py, googleapis-common-protos, fonttools, flatbuffers, cycler, clang, uritemplate, typeguard, tqdm, tensorflow-metadata, tensorflow-hub, tensorflow, tabulate, scikit-learn, regex, pytz, python-slugify, promise, portalocker, matplotlib, google-auth-httplib2, google-api-core, future, docopt, dm-tree, dill, colorama, attrs, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-io-gcs-filesystem, tensorflow-datasets, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, pydot, pycocotools, pyarrow, py-cpuinfo, psutil, pandas, orjson, opencv-python-headless, opencv-python, oauth2client, kaggle, hdfs, google-api-python-client, gin-config, fastavro, crcmod, avro-python3, tf-models-official, tensorflow-io, lxml, lvis, contextlib2, apache-beam, object-detection\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.3.3\n",
      "    Uninstalling google-auth-2.3.3:\n",
      "      Successfully uninstalled google-auth-2.3.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.4\n",
      "    Uninstalling numpy-1.21.4:\n",
      "      Successfully uninstalled numpy-1.21.4\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.0.0\n",
      "    Uninstalling absl-py-1.0.0:\n",
      "      Successfully uninstalled absl-py-1.0.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.13.3\n",
      "    Uninstalling wrapt-1.13.3:\n",
      "      Successfully uninstalled wrapt-1.13.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.0\n",
      "    Uninstalling typing-extensions-4.0.0:\n",
      "      Successfully uninstalled typing-extensions-4.0.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.7.0\n",
      "    Uninstalling keras-2.7.0:\n",
      "      Successfully uninstalled keras-2.7.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.6.0\n",
      "    Uninstalling h5py-3.6.0:\n",
      "      Successfully uninstalled h5py-3.6.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.22.0\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.22.0:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.22.0\n",
      "Successfully installed absl-py-0.12.0 apache-beam-2.34.0 attrs-21.2.0 avro-python3-1.9.2.1 clang-5.0 colorama-0.4.4 contextlib2-21.6.0 crcmod-1.7 cycler-0.11.0 dill-0.3.1.1 dm-tree-0.1.6 docopt-0.6.2 fastavro-1.4.7 flatbuffers-1.12 fonttools-4.28.2 future-0.18.2 gin-config-0.5.0 google-api-core-2.2.2 google-api-python-client-2.31.0 google-auth-1.35.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.53.0 h5py-3.1.0 hdfs-2.6.0 httplib2-0.19.1 joblib-1.1.0 kaggle-1.5.12 keras-2.6.0 kiwisolver-1.3.2 lvis-0.5.3 lxml-4.6.4 matplotlib-3.5.0 numpy-1.19.5 oauth2client-4.1.3 object-detection-0.1 opencv-python-4.5.4.60 opencv-python-headless-4.5.4.60 orjson-3.6.4 packaging-21.3 pandas-1.3.4 pillow-8.4.0 portalocker-2.3.2 promise-2.3 psutil-5.8.0 py-cpuinfo-8.0.0 pyarrow-5.0.0 pycocotools-2.0.2 pydot-1.4.2 pymongo-3.12.1 pyparsing-2.4.7 python-slugify-5.0.2 pytz-2021.3 pyyaml-6.0 regex-2021.11.10 sacrebleu-2.0.0 scikit-learn-1.0.1 scipy-1.7.2 sentencepiece-0.1.96 seqeval-1.2.2 setuptools-scm-6.3.2 six-1.15.0 tabulate-0.8.9 tensorboard-2.6.0 tensorflow-2.6.2 tensorflow-addons-0.15.0 tensorflow-datasets-4.4.0 tensorflow-estimator-2.6.0 tensorflow-hub-0.12.0 tensorflow-io-0.21.0 tensorflow-io-gcs-filesystem-0.21.0 tensorflow-metadata-1.4.0 tensorflow-model-optimization-0.7.0 tensorflow-text-2.6.0 text-unidecode-1.3 tf-models-official-2.6.0 tf-slim-1.1.0 threadpoolctl-3.0.0 tomli-1.2.2 tqdm-4.62.3 typeguard-2.13.2 typing-extensions-3.7.4.3 uritemplate-4.1.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyrPaXSxuWmI",
    "outputId": "e405f01f-4001-45e3-ece0-adad9d7b5d48"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# From within TensorFlow/models/research/\n",
    "!python object_detection/builders/model_builder_tf2_test.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-11-24 16:20:33.729906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-24 16:20:33.753430: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-11-24 16:20:33.753454: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Running tests under Python 3.9.5: /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/bin/python\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "2021-11-24 16:20:33.761011: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/venv/lib/python3.9/site-packages/object_detection/builders/model_builder.py:1100: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(('Building experimental DeepMAC meta-arch.'\n",
      "W1124 16:20:33.944539 139727438473024 model_builder.py:1100] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 0.38s\n",
      "I1124 16:20:34.131242 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 0.38s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.46s\n",
      "I1124 16:20:34.592060 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.46s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.19s\n",
      "I1124 16:20:34.787011 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.19s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.18s\n",
      "I1124 16:20:34.967095 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.18s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.25s\n",
      "I1124 16:20:36.218513 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.25s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "I1124 16:20:36.219234 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
      "I1124 16:20:36.234970 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
      "I1124 16:20:36.245513 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
      "I1124 16:20:36.258369 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.07s\n",
      "I1124 16:20:36.329362 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.07s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.07s\n",
      "I1124 16:20:36.396614 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.07s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.07s\n",
      "I1124 16:20:36.466835 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.07s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.07s\n",
      "I1124 16:20:36.536463 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.07s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.07s\n",
      "I1124 16:20:36.603652 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.07s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.02s\n",
      "I1124 16:20:36.622536 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.02s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I1124 16:20:36.748204 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I1124 16:20:36.748304 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
      "I1124 16:20:36.748357 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 3\n",
      "I1124 16:20:36.749840 139727438473024 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 16:20:36.760618 139727438473024 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 16:20:36.760693 139727438473024 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 16:20:36.799637 139727438473024 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 16:20:36.799742 139727438473024 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 16:20:36.905396 139727438473024 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 16:20:36.905495 139727438473024 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1124 16:20:37.007222 139727438473024 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1124 16:20:37.007322 139727438473024 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1124 16:20:37.163927 139727438473024 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1124 16:20:37.164023 139727438473024 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1124 16:20:37.321485 139727438473024 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1124 16:20:37.321585 139727438473024 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1124 16:20:37.624056 139727438473024 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1124 16:20:37.624154 139727438473024 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1124 16:20:37.678293 139727438473024 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1124 16:20:37.703229 139727438473024 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 16:20:37.743482 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I1124 16:20:37.743581 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 88\n",
      "I1124 16:20:37.743630 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 4\n",
      "I1124 16:20:37.744776 139727438473024 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 16:20:37.756399 139727438473024 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 16:20:37.756515 139727438473024 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 16:20:37.840137 139727438473024 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 16:20:37.840235 139727438473024 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 16:20:37.991532 139727438473024 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 16:20:37.991628 139727438473024 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1124 16:20:38.145941 139727438473024 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1124 16:20:38.146040 139727438473024 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1124 16:20:38.355743 139727438473024 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1124 16:20:38.355868 139727438473024 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1124 16:20:38.570072 139727438473024 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1124 16:20:38.570172 139727438473024 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1124 16:20:38.840880 139727438473024 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1124 16:20:38.840985 139727438473024 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1124 16:20:38.954985 139727438473024 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1124 16:20:38.979552 139727438473024 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 16:20:39.024343 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
      "I1124 16:20:39.024515 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 112\n",
      "I1124 16:20:39.024599 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 5\n",
      "I1124 16:20:39.025983 139727438473024 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 16:20:39.036871 139727438473024 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 16:20:39.036961 139727438473024 efficientnet_model.py:147] round_filter input=16 output=16\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I1124 16:20:39.118071 139727438473024 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 16:20:39.118166 139727438473024 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 16:20:39.271804 139727438473024 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 16:20:39.271915 139727438473024 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1124 16:20:39.426880 139727438473024 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1124 16:20:39.426980 139727438473024 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I1124 16:20:39.643697 139727438473024 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I1124 16:20:39.643795 139727438473024 efficientnet_model.py:147] round_filter input=112 output=120\n",
      "I1124 16:20:39.855783 139727438473024 efficientnet_model.py:147] round_filter input=112 output=120\n",
      "I1124 16:20:39.855882 139727438473024 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I1124 16:20:40.218544 139727438473024 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I1124 16:20:40.218655 139727438473024 efficientnet_model.py:147] round_filter input=320 output=352\n",
      "I1124 16:20:40.335053 139727438473024 efficientnet_model.py:147] round_filter input=1280 output=1408\n",
      "I1124 16:20:40.361093 139727438473024 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 16:20:40.405017 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
      "I1124 16:20:40.405119 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 160\n",
      "I1124 16:20:40.405170 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 6\n",
      "I1124 16:20:40.406347 139727438473024 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I1124 16:20:40.417613 139727438473024 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I1124 16:20:40.417689 139727438473024 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 16:20:40.499850 139727438473024 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 16:20:40.499947 139727438473024 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1124 16:20:40.655341 139727438473024 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1124 16:20:40.655443 139727438473024 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1124 16:20:40.815347 139727438473024 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1124 16:20:40.815448 139727438473024 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I1124 16:20:41.081317 139727438473024 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I1124 16:20:41.081421 139727438473024 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I1124 16:20:41.351313 139727438473024 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I1124 16:20:41.351416 139727438473024 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I1124 16:20:41.687978 139727438473024 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I1124 16:20:41.688081 139727438473024 efficientnet_model.py:147] round_filter input=320 output=384\n",
      "I1124 16:20:41.807033 139727438473024 efficientnet_model.py:147] round_filter input=1280 output=1536\n",
      "I1124 16:20:41.831734 139727438473024 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 16:20:41.880590 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
      "I1124 16:20:41.880692 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 224\n",
      "I1124 16:20:41.880753 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 7\n",
      "I1124 16:20:41.881973 139727438473024 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1124 16:20:41.892609 139727438473024 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1124 16:20:41.892690 139727438473024 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 16:20:41.975096 139727438473024 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 16:20:41.975194 139727438473024 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1124 16:20:42.185844 139727438473024 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1124 16:20:42.185944 139727438473024 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I1124 16:20:42.400044 139727438473024 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I1124 16:20:42.400145 139727438473024 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I1124 16:20:42.716763 139727438473024 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I1124 16:20:42.716868 139727438473024 efficientnet_model.py:147] round_filter input=112 output=160\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I1124 16:20:43.092824 139727438473024 efficientnet_model.py:147] round_filter input=112 output=160\n",
      "I1124 16:20:43.093118 139727438473024 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I1124 16:20:43.834520 139727438473024 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I1124 16:20:43.834628 139727438473024 efficientnet_model.py:147] round_filter input=320 output=448\n",
      "I1124 16:20:43.960558 139727438473024 efficientnet_model.py:147] round_filter input=1280 output=1792\n",
      "I1124 16:20:43.990745 139727438473024 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 16:20:44.051167 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
      "I1124 16:20:44.051269 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 288\n",
      "I1124 16:20:44.051319 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 7\n",
      "I1124 16:20:44.052455 139727438473024 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1124 16:20:44.062622 139727438473024 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1124 16:20:44.062690 139727438473024 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 16:20:44.186604 139727438473024 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 16:20:44.186700 139727438473024 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1124 16:20:44.448208 139727438473024 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1124 16:20:44.448308 139727438473024 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I1124 16:20:44.710246 139727438473024 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I1124 16:20:44.710343 139727438473024 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I1124 16:20:45.234950 139727438473024 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I1124 16:20:45.235132 139727438473024 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I1124 16:20:45.743515 139727438473024 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I1124 16:20:45.743710 139727438473024 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I1124 16:20:46.327315 139727438473024 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I1124 16:20:46.327420 139727438473024 efficientnet_model.py:147] round_filter input=320 output=512\n",
      "I1124 16:20:46.537146 139727438473024 efficientnet_model.py:147] round_filter input=1280 output=2048\n",
      "I1124 16:20:46.573254 139727438473024 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 16:20:46.639767 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
      "I1124 16:20:46.639867 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I1124 16:20:46.639917 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 8\n",
      "I1124 16:20:46.641069 139727438473024 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I1124 16:20:46.651614 139727438473024 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I1124 16:20:46.651708 139727438473024 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1124 16:20:46.779371 139727438473024 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1124 16:20:46.779469 139727438473024 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1124 16:20:47.318011 139727438473024 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1124 16:20:47.318125 139727438473024 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I1124 16:20:47.670573 139727438473024 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I1124 16:20:47.670693 139727438473024 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I1124 16:20:48.125148 139727438473024 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I1124 16:20:48.125246 139727438473024 efficientnet_model.py:147] round_filter input=112 output=200\n",
      "I1124 16:20:48.628364 139727438473024 efficientnet_model.py:147] round_filter input=112 output=200\n",
      "I1124 16:20:48.628520 139727438473024 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I1124 16:20:49.314313 139727438473024 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I1124 16:20:49.314429 139727438473024 efficientnet_model.py:147] round_filter input=320 output=576\n",
      "I1124 16:20:49.522390 139727438473024 efficientnet_model.py:147] round_filter input=1280 output=2304\n",
      "I1124 16:20:49.553181 139727438473024 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I1124 16:20:49.635043 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
      "I1124 16:20:49.635157 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I1124 16:20:49.635211 139727438473024 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 8\n",
      "I1124 16:20:49.636547 139727438473024 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I1124 16:20:49.650093 139727438473024 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I1124 16:20:49.650202 139727438473024 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1124 16:20:49.876628 139727438473024 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1124 16:20:49.876794 139727438473024 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I1124 16:20:50.354787 139727438473024 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I1124 16:20:50.354887 139727438473024 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I1124 16:20:50.736765 139727438473024 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I1124 16:20:50.736868 139727438473024 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I1124 16:20:51.416508 139727438473024 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I1124 16:20:51.416612 139727438473024 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I1124 16:20:52.028579 139727438473024 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I1124 16:20:52.028681 139727438473024 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I1124 16:20:52.919842 139727438473024 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I1124 16:20:52.919975 139727438473024 efficientnet_model.py:147] round_filter input=320 output=640\n",
      "I1124 16:20:53.266575 139727438473024 efficientnet_model.py:147] round_filter input=1280 output=2560\n",
      "I1124 16:20:53.300486 139727438473024 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 16.77s\n",
      "I1124 16:20:53.391729 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 16.77s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "I1124 16:20:53.398088 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "I1124 16:20:53.399313 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "I1124 16:20:53.399618 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "I1124 16:20:53.400691 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTF2Test.test_session\n",
      "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "I1124 16:20:53.401667 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "I1124 16:20:53.401937 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "I1124 16:20:53.402688 139727438473024 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 24 tests in 19.647s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjlR4lsmuWpE",
    "outputId": "fb38e7a6-f962-4187-ecae-0e489c6fe5d0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the workplace"
   ],
   "metadata": {
    "id": "dc382MCGBuDl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%cd ..\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TxPzE3UKHlt",
    "outputId": "59eab01b-b9fb-47d3-db4f-578705260015"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
    "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
    "!ls /mydrive"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'55416691_Oris Pratama_acc skripsi.pdf.ribd'\n",
      "'55416691_Oris Pratama_Agenda bimbingan.pdf.ribd'\n",
      "'Bahasa Indonesia dan Generasi Kini.gdoc'\n",
      "'Colab Notebooks'\n",
      "'Dokumentasi for OCR.gdoc'\n",
      "'File PI'\n",
      "'Form Komitmen dan Form Pertanggungjawaban PRODTS2021. [Peserta].gdoc.gdoc'\n",
      " img027.pdf\n",
      " img028.jpeg\n",
      " img028.pdf\n",
      " LPR.gslides.gslides\n",
      "'my file'\n",
      "'Oris Pratama – UG – TI – Javascript-Android Java.pdf.ribd'\n",
      " P_20201218_131950.jpg.ribd\n",
      " report.csv\n",
      " report.gsheet\n",
      "'Research LPR.gdoc.gdoc'\n",
      " Skripsi\n",
      "'Skripsi SK Teknik Informatika 2020_Lintang_edited.xlsx.ribd'\n",
      "'Standup Special Pandji'\n",
      " tensor\n",
      "'Untitled document.gdoc'\n",
      " yolov3\n",
      " yolov4\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CR_TDc5KXtd",
    "outputId": "d7dc9fe5-c74b-4388-c3ea-8b2f9aa3566c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import os\n",
    "repo_url = 'https://github.com/orizzz/tf_od_tutorial'\n",
    "%cd /content\n",
    "\n",
    "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
    "\n",
    "!git clone {repo_url}\n",
    "%cd {repo_dir_path}\n",
    "!git pull"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n",
      "Cloning into 'tf_od_tutorial'...\n",
      "remote: Enumerating objects: 3954, done.\u001b[K\n",
      "remote: Total 3954 (delta 0), reused 0 (delta 0), pack-reused 3954\u001b[K\n",
      "Receiving objects: 100% (3954/3954), 281.91 MiB | 33.85 MiB/s, done.\n",
      "Resolving deltas: 100% (1942/1942), done.\n",
      "Checking out files: 100% (3818/3818), done.\n",
      "/content/tf_od_tutorial\n",
      "Already up to date.\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMXzIKmvIuqB",
    "outputId": "80397f69-b490-4312-997a-684c0f5aa686"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/                       README.md\n",
      "\u001b[01;34mdeploy\u001b[0m/                     requirements.txt\n",
      "generate_tfrecord.py        resize_images.py\n",
      "label_map.pbtxt             spn_tensorflow_training.ipynb\n",
      "LICENSE                     tensorflow_object_detection_training_colab.ipynb\n",
      "local_inference_test.ipynb  \u001b[01;34mtest\u001b[0m/\n",
      "local_inference_test.py     xml_to_csv.py\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNxbelQmJDVV",
    "outputId": "ed8ec5f3-555a-4b88-d0a1-ccdb3efb8f9c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the necessary files"
   ],
   "metadata": {
    "id": "F8qJbV1uauAG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research'"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%mkdir -p /mydrive/tensor/training_demo\n",
    "%cd /mydrive/tensor/training_demo"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2bohyWtB6Es",
    "outputId": "bff44493-db33-417d-8bed-db65d7db0c5d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pwd\n",
    "!ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8YaBALaCITV",
    "outputId": "9b6ee546-a2db-4cf8-c933-84ccb5bce01c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%mkdir -p annotations\n",
    "%mkdir -p exported_models\n",
    "%mkdir -p images\n",
    "%mkdir -p pre-trained-models\n",
    "%ls\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/  \u001b[01;34mexported_models\u001b[0m/  \u001b[01;34mimages\u001b[0m/  \u001b[01;34mpre-trained-models\u001b[0m/\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pesLUuIzCl4X",
    "outputId": "769322fd-9aa6-46e5-c199-87611dafb8dc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cp -r {repo_dir_path}/data/images /content/training_demo/"
   ],
   "outputs": [],
   "metadata": {
    "id": "6aQSRxusJg9t"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cd /content/training_demo/images\n",
    "%ls"
   ],
   "outputs": [],
   "metadata": {
    "id": "8u5NF3LoaZs5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cd /content/training_demo/\n",
    "%ls"
   ],
   "outputs": [],
   "metadata": {
    "id": "aM7hbvyua_lV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cp -r {repo_dir_path}/generate_tfrecord.py /mydrive/tensor/training_demo\n",
    "%ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/      generate_tfrecord.py  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/  \u001b[01;34mimages\u001b[0m/\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3wQwqGoaw5h",
    "outputId": "5285b372-0979-4859-bf0a-26b35c5ad303"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cp -r {repo_dir_path}/label_map.pbtxt /mydrive/tensor/training_demo/annotations\n",
    "%ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/      generate_tfrecord.py  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/  \u001b[01;34mimages\u001b[0m/\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A83mzmtlDi5z",
    "outputId": "7f043653-5167-436d-f158-993fe07d1ad0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "%cd /mydrive/tensor/training_demo/pre-trained-models "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo/pre-trained-models\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gL4BBRoZuWr8",
    "outputId": "736fbd0a-1352-4f2f-cbeb-34013980d5cf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-11-24 06:44:53--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.157.128, 2404:6800:4008:c13::80\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.157.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 46042990 (44M) [application/x-tar]\n",
      "Saving to: ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz’\n",
      "\n",
      "ssd_mobilenet_v2_32 100%[===================>]  43.91M  19.4MB/s    in 2.3s    \n",
      "\n",
      "2021-11-24 06:44:57 (19.4 MB/s) - ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz’ saved [46042990/46042990]\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MItGLVY3uWu8",
    "outputId": "c5821842-e5d7-42ef-da17-11805e2fdc6f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "!tar -xvf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/checkpoint\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0.index\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/saved_model.pb\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/variables.index\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzlPcDPLuWye",
    "outputId": "62af3c61-af9f-4c3a-dcb1-b0c28607bb8a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/gdrive/My Drive/tensor/training_demo/pre-trained-models'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Vf49zVcAtNJB",
    "outputId": "23514e80-8763-4dca-a9e5-4f43afe31c97"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cd /mydrive/tensor/training_demo"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFA0L4rmyGae",
    "outputId": "a9d9c336-3d6a-4526-85cd-932873d1e124"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cp -r {repo_dir_path}/data/models /mydrive/tensor/training_demo"
   ],
   "outputs": [],
   "metadata": {
    "id": "dxveA1zpALrB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                          \u001b[01;34mimages\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/                                      \u001b[01;34mmodels\u001b[0m/\n",
      "faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "generate_tfrecord.py\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pxst7lPT0Sby",
    "outputId": "f07617ef-28cd-4994-fabc-31c451494b0e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create train data:\n",
    "!python generate_tfrecord.py -x {repo_dir_path}/data/images/train -l /mydrive/tensor/training_demo/annotations/label_map.pbtxt -o /mydrive/tensor/training_demo/annotations/train.record\n",
    "\n",
    "# Create test data:\n",
    "!python generate_tfrecord.py -x {repo_dir_path}/data/images/test -l /mydrive/tensor/training_demo/annotations/label_map.pbtxt -o /mydrive/tensor/training_demo/annotations/test.record"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully created the TFRecord file: /mydrive/tensor/training_demo/annotations/train.record\n",
      "Successfully created the TFRecord file: /mydrive/tensor/training_demo/annotations/test.record\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hU2lVZfzyuar",
    "outputId": "2f8d192e-f828-4469-c695-e2acb46cccaa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/gdrive/My Drive/tensor/training_demo'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "MfHABHxi56kM",
    "outputId": "10c0a8e4-80b6-4e44-a567-6756bb5ca777"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                          \u001b[01;34mimages\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/                                      \u001b[01;34mmodels\u001b[0m/\n",
      "faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "generate_tfrecord.py\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQO8P8xLy6bS",
    "outputId": "768e5011-d61d-455f-9b9f-e3cb278d0f9c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cp -r /content/models/research/object_detection/model_main_tf2.py /mydrive/tensor/training_demo\n",
    "%ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                          \u001b[01;34mimages\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/                                      model_main_tf2.py\n",
      "faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz  \u001b[01;34mmodels\u001b[0m/\n",
      "generate_tfrecord.py                                  \u001b[01;34mpre-trained-models\u001b[0m/\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-XrMHcKOVWD0",
    "outputId": "f30cb38b-d622-498d-aedc-43b886338931"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start training\n"
   ],
   "metadata": {
    "id": "_JQFLVmCa4Yo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "cd /mydrive/tensor/training_demo"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uO_IHATIEqV1",
    "outputId": "7a85ed62-fc90-4d9d-a7d2-897af851b8d9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "!python model_main_tf2.py --model_dir=/mydrive/tensor/training_demo/models/spn_ssd_mobilenet_v2_320x320 --pipeline_config_path=/mydrive/tensor/training_demo/models/spn_ssd_mobilenet_v2_320x320/pipeline.config"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-11-24 04:50:22.020668: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "I1124 04:50:22.026642 140015312131968 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\n",
      "I1124 04:50:22.551321 140015312131968 config_util.py:552] Maybe overwriting train_steps: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1124 04:50:22.551607 140015312131968 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W1124 04:50:22.595784 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/mydrive/tensor/training_demo/annotations/train.record']\n",
      "I1124 04:50:22.741549 140015312131968 dataset_builder.py:163] Reading unweighted datasets: ['/mydrive/tensor/training_demo/annotations/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/mydrive/tensor/training_demo/annotations/train.record']\n",
      "I1124 04:50:22.742175 140015312131968 dataset_builder.py:80] Reading record datasets for input file: ['/mydrive/tensor/training_demo/annotations/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1124 04:50:22.742341 140015312131968 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1124 04:50:22.742489 140015312131968 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "W1124 04:50:22.746255 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1124 04:50:22.769541 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1124 04:50:31.836089 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1124 04:50:35.647020 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1124 04:50:55.902511 140011229746944 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "W1124 04:51:09.800204 140011229746944 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W1124 04:51:17.665125 140011229746944 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2021-11-24 04:51:47.954507: E tensorflow/stream_executor/cuda/cuda_dnn.cc:362] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2021-11-24 04:51:47.958479: E tensorflow/stream_executor/cuda/cuda_dnn.cc:362] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "Traceback (most recent call last):\n",
      "  File \"model_main_tf2.py\", line 115, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"model_main_tf2.py\", line 112, in main\n",
      "    record_summaries=FLAGS.record_summaries)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 603, in train_loop\n",
      "    train_input, unpad_groundtruth_tensors)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 394, in load_fine_tune_checkpoint\n",
      "    _ensure_model_is_built(model, input_dataset, unpad_groundtruth_tensors)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 176, in _ensure_model_is_built\n",
      "    labels,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1286, in run\n",
      "    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 2849, in call_for_each_replica\n",
      "    return self._call_for_each_replica(fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 671, in _call_for_each_replica\n",
      "    self._container_strategy(), fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_run.py\", line 86, in call_for_each_replica\n",
      "    return wrapped(args, kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 950, in _call\n",
      "    return self._stateless_fn(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3040, in __call__\n",
      "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1964, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 596, in call\n",
      "    ctx=ctx)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\n",
      "  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
      "\t [[node model/conv1_conv/Conv2D (defined at /local/lib/python3.7/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1346) ]]\n",
      "\t [[Loss/RPNLoss/BalancedPositiveNegativeSampler_1/Cast_8/_588]]\n",
      "  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
      "\t [[node model/conv1_conv/Conv2D (defined at /local/lib/python3.7/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1346) ]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference__dummy_computation_fn_44910]\n",
      "\n",
      "Errors may have originated from an input operation.\n",
      "Input Source operations connected to node model/conv1_conv/Conv2D:\n",
      " model/lambda/Pad (defined at /local/lib/python3.7/dist-packages/object_detection/models/keras_models/resnet_v1.py:51)\n",
      "\n",
      "Input Source operations connected to node model/conv1_conv/Conv2D:\n",
      " model/lambda/Pad (defined at /local/lib/python3.7/dist-packages/object_detection/models/keras_models/resnet_v1.py:51)\n",
      "\n",
      "Function call stack:\n",
      "_dummy_computation_fn -> _dummy_computation_fn\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNVwlSCq9pr1",
    "outputId": "209122fb-9ff2-439f-ab03-2f45a1419851"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "vT8TkYh-OISw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/training_demo'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 63
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TlRqjT4AFOhv",
    "outputId": "a77528bd-f60a-4846-cf1a-f2b7eb8d3513"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python exporter_main_v2.py --input_type image_tensor --pipeline_config_path /content/training_demo/models/spn_ssd_mobilenet_v2_320x320/pipeline.config --trained_checkpoint_dir /content/training_demo/models/spn_ssd_mobilenet_v2_320x320 --output_directory /content/training_demo/exported_models/spn_detection_model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "python3: can't open file 'exporter_main_v2.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeI2URnR9zhw",
    "outputId": "2defe90f-c72b-49e4-a7c3-e8d6cdf145b3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inferencing My Trained Models"
   ],
   "metadata": {
    "id": "GIIKwpxtGAhK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Object Detection (On Image) From TF2 Saved Model\n",
    "=====================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import argparse\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# PROVIDE PATH TO IMAGE DIRECTORY\n",
    "IMAGE_PATHS = '/content/training_demo/images/train/image1.jpg'\n",
    "\n",
    "\n",
    "# PROVIDE PATH TO MODEL DIRECTORY\n",
    "PATH_TO_MODEL_DIR = '/content/training_demo/exported_models/my_model'\n",
    "\n",
    "# PROVIDE PATH TO LABEL MAP\n",
    "PATH_TO_LABELS = '/content/training_demo/annotations/label_map.pbtxt'\n",
    "\n",
    "# PROVIDE THE MINIMUM CONFIDENCE THRESHOLD\n",
    "MIN_CONF_THRESH = float(0.60)\n",
    "\n",
    "# LOAD THE MODEL\n",
    "\n",
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))\n",
    "\n",
    "# LOAD LABEL MAP DATA FOR PLOTTING\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Running inference for {}... '.format(IMAGE_PATHS), end='')\n",
    "\n",
    "image = cv2.imread(IMAGE_PATHS)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_expanded = np.expand_dims(image_rgb, axis=0)\n",
    "\n",
    "# The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "input_tensor = tf.convert_to_tensor(image)\n",
    "# The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "# input_tensor = np.expand_dims(image_np, 0)\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "# All outputs are batches tensors.\n",
    "# Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "# We're only interested in the first num_detections.\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "               for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "image_with_detections = image.copy()\n",
    "\n",
    "# SET MIN_SCORE_THRESH BASED ON YOU MINIMUM THRESHOLD FOR DETECTIONS\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_with_detections,\n",
    "      detections['detection_boxes'],\n",
    "      detections['detection_classes'],\n",
    "      detections['detection_scores'],\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=0.5,\n",
    "      agnostic_mode=False)\n",
    "\n",
    "print('Done')\n",
    "# DISPLAYS OUTPUT IMAGE\n",
    "cv2_imshow(image_with_detections)\n",
    "# CLOSES WINDOW ONCE KEY IS PRESSED\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "spZJ4ms3FqRT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "k0KheEfPGYhO"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of spn tensorflow training di akun spn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}