{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "GnVG7OmvsAOR",
    "outputId": "53d6d057-7af6-4b30-e43a-fba64f321ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 471 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 744 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.22.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 583 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting numpy>=1.14.5\n",
      "  Downloading numpy-1.21.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 442 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel<1.0,>=0.32.0\n",
      "  Using cached wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 376 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.13.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 869 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Using cached libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4 MB)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-59.2.0-py3-none-any.whl (952 kB)\n",
      "\u001b[K     |████████████████████████████████| 952 kB 403 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 529 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, six, setuptools, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.7 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.8.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 numpy-1.21.4 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-59.2.0 six-1.16.0 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0 typing-extensions-4.0.0 urllib3-1.26.7 werkzeug-2.0.2 wheel-0.37.0 wrapt-1.13.3 zipp-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow --ignore-installed --upgrade \n",
    "#tensorflow-gpu==2.4.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- ----------\n",
      "absl-py                       0.12.0\n",
      "apache-beam                   2.34.0\n",
      "astunparse                    1.6.3\n",
      "attrs                         21.2.0\n",
      "avro-python3                  1.9.2.1\n",
      "backcall                      0.2.0\n",
      "cachetools                    4.2.4\n",
      "certifi                       2021.10.8\n",
      "charset-normalizer            2.0.7\n",
      "clang                         5.0\n",
      "colorama                      0.4.4\n",
      "contextlib2                   21.6.0\n",
      "crcmod                        1.7\n",
      "cycler                        0.11.0\n",
      "Cython                        0.29.24\n",
      "debugpy                       1.5.1\n",
      "decorator                     5.1.0\n",
      "dill                          0.3.1.1\n",
      "dm-tree                       0.1.6\n",
      "docopt                        0.6.2\n",
      "entrypoints                   0.3\n",
      "fastavro                      1.4.7\n",
      "flatbuffers                   1.12\n",
      "fonttools                     4.28.2\n",
      "future                        0.18.2\n",
      "gast                          0.4.0\n",
      "gin-config                    0.5.0\n",
      "google-api-core               2.2.2\n",
      "google-api-python-client      2.31.0\n",
      "google-auth                   1.35.0\n",
      "google-auth-httplib2          0.1.0\n",
      "google-auth-oauthlib          0.4.6\n",
      "google-pasta                  0.2.0\n",
      "googleapis-common-protos      1.53.0\n",
      "grpcio                        1.42.0\n",
      "h5py                          3.1.0\n",
      "hdfs                          2.6.0\n",
      "httplib2                      0.19.1\n",
      "idna                          3.3\n",
      "importlib-metadata            4.8.2\n",
      "ipykernel                     6.5.1\n",
      "ipython                       7.29.0\n",
      "jedi                          0.18.1\n",
      "joblib                        1.1.0\n",
      "jupyter-client                7.1.0\n",
      "jupyter-core                  4.9.1\n",
      "kaggle                        1.5.12\n",
      "keras                         2.6.0\n",
      "Keras-Preprocessing           1.1.2\n",
      "kiwisolver                    1.3.2\n",
      "libclang                      12.0.0\n",
      "lvis                          0.5.3\n",
      "lxml                          4.6.4\n",
      "Markdown                      3.3.6\n",
      "matplotlib                    3.5.0\n",
      "matplotlib-inline             0.1.3\n",
      "nest-asyncio                  1.5.1\n",
      "numpy                         1.19.5\n",
      "oauth2client                  4.1.3\n",
      "oauthlib                      3.1.1\n",
      "object-detection              0.1\n",
      "opencv-python                 4.5.4.60\n",
      "opencv-python-headless        4.5.4.60\n",
      "opt-einsum                    3.3.0\n",
      "orjson                        3.6.4\n",
      "packaging                     21.3\n",
      "pandas                        1.3.4\n",
      "parso                         0.8.2\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        8.4.0\n",
      "pip                           21.3.1\n",
      "pkg_resources                 0.0.0\n",
      "portalocker                   2.3.2\n",
      "promise                       2.3\n",
      "prompt-toolkit                3.0.22\n",
      "protobuf                      3.19.1\n",
      "psutil                        5.8.0\n",
      "ptyprocess                    0.7.0\n",
      "py-cpuinfo                    8.0.0\n",
      "pyarrow                       5.0.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycocotools                   2.0.2\n",
      "pydot                         1.4.2\n",
      "Pygments                      2.10.0\n",
      "pymongo                       3.12.1\n",
      "pyparsing                     2.4.7\n",
      "python-dateutil               2.8.2\n",
      "python-slugify                5.0.2\n",
      "pytz                          2021.3\n",
      "PyYAML                        6.0\n",
      "pyzmq                         22.3.0\n",
      "regex                         2021.11.10\n",
      "requests                      2.26.0\n",
      "requests-oauthlib             1.3.0\n",
      "rsa                           4.7.2\n",
      "sacrebleu                     2.0.0\n",
      "scikit-learn                  1.0.1\n",
      "scipy                         1.7.2\n",
      "sentencepiece                 0.1.96\n",
      "seqeval                       1.2.2\n",
      "setuptools                    59.2.0\n",
      "setuptools-scm                6.3.2\n",
      "six                           1.15.0\n",
      "tabulate                      0.8.9\n",
      "tensorboard                   2.6.0\n",
      "tensorboard-data-server       0.6.1\n",
      "tensorboard-plugin-wit        1.8.0\n",
      "tensorflow                    2.6.2\n",
      "tensorflow-addons             0.15.0\n",
      "tensorflow-datasets           4.4.0\n",
      "tensorflow-estimator          2.6.0\n",
      "tensorflow-hub                0.12.0\n",
      "tensorflow-io                 0.21.0\n",
      "tensorflow-io-gcs-filesystem  0.21.0\n",
      "tensorflow-metadata           1.4.0\n",
      "tensorflow-model-optimization 0.7.0\n",
      "tensorflow-text               2.6.0\n",
      "termcolor                     1.1.0\n",
      "text-unidecode                1.3\n",
      "tf-models-official            2.6.0\n",
      "tf-slim                       1.1.0\n",
      "threadpoolctl                 3.0.0\n",
      "tomli                         1.2.2\n",
      "tornado                       6.1\n",
      "tqdm                          4.62.3\n",
      "traitlets                     5.1.1\n",
      "typeguard                     2.13.2\n",
      "typing-extensions             3.7.4.3\n",
      "uritemplate                   4.1.1\n",
      "urllib3                       1.26.7\n",
      "wcwidth                       0.2.5\n",
      "Werkzeug                      2.0.2\n",
      "wheel                         0.37.0\n",
      "wrapt                         1.12.1\n",
      "zipp                          3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oeyP0aYes9Zs",
    "outputId": "446538fc-01d0-4b95-c156-0347649feb24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow_version : 2.7.0\n",
      "cuda_version : 11.2\n",
      "cudnn_version : 8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"tensorflow_version :\",tf.__version__)\n",
    "\n",
    "sys_details = tf.sysconfig.get_build_info()\n",
    "cuda_version = sys_details[\"cuda_version\"]\n",
    "print(\"cuda_version :\",cuda_version)\n",
    "  \n",
    "cudnn_version = sys_details[\"cudnn_version\"]  \n",
    "print(\"cudnn_version :\", cudnn_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "q8AOOLDGAsPW",
    "outputId": "0e957e32-a99b-4bba-80a9-9490005ef83e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Sun_Feb_14_21:12:58_PST_2021\n",
      "Cuda compilation tools, release 11.2, V11.2.152\n",
      "Build cuda_11.2.r11.2/compiler.29618528_0\n",
      "Failed to initialize NVML: Driver/library version mismatch\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEGpXLl3tQVX"
   },
   "source": [
    "## **Cloning TFOD 2.0 Github**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kFkdXoEltLY9",
    "outputId": "7eb1d02e-d54f-4ebc-9ad7-da18d9cf6fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 67421, done.\u001b[K\n",
      "remote: Total 67421 (delta 0), reused 0 (delta 0), pack-reused 67421\u001b[K\n",
      "Receiving objects: 100% (67421/67421), 576.20 MiB | 200.00 KiB/s, done.\n",
      "Resolving deltas: 100% (47324/47324), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "collapsed": true,
    "id": "aJ4YzpQ4tMlz",
    "outputId": "e0f424d7-938e-4707-c905-7d3d17b7aec5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6ZwI0sTdtMsc",
    "outputId": "991074c3-5878-41d2-bba0-98c65e07cdb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research\n"
     ]
    }
   ],
   "source": [
    "cd models/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "collapsed": true,
    "id": "SK5RyNz9tMvZ",
    "outputId": "436d4afe-bdfc-4c26-b61d-552adb5db5fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "NozH3MfAtMyR"
   },
   "outputs": [],
   "source": [
    "!protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "alOqL7ortM1F",
    "outputId": "c872534d-2d0d-4bb4-fbdf-98d009ea6699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cocoapi'...\n",
      "remote: Enumerating objects: 975, done.\u001b[K\n",
      "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
      "Receiving objects: 100% (975/975), 11.72 MiB | 305.00 KiB/s, done.\n",
      "Resolving deltas: 100% (576/576), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cocodataset/cocoapi.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0XRlMiuEtM4R",
    "outputId": "07c01d9b-d900-46de-9cb5-a5d7f51fe803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research/cocoapi/PythonAPI\n"
     ]
    }
   ],
   "source": [
    "cd cocoapi/PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cython\n",
      "  Using cached Cython-0.29.24-cp39-cp39-manylinux1_x86_64.whl (1.9 MB)\n",
      "Installing collected packages: cython\n",
      "Successfully installed cython-0.29.24\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PZ-xj6MUtM7U",
    "outputId": "daad3ed7-9a5e-4d54-9299-5d324fb1d954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python setup.py build_ext --inplace\n",
      "running build_ext\n",
      "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
      "/home/oriz_1st/.local/lib/python3.9/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "building 'pycocotools._mask' extension\n",
      "creating build\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-3.9\n",
      "creating build/temp.linux-x86_64-3.9/pycocotools\n",
      "gcc -pthread -B /home/oriz_1st/anaconda3/envs/tensorflow/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/oriz_1st/anaconda3/envs/tensorflow/include -I/home/oriz_1st/anaconda3/envs/tensorflow/include -fPIC -O2 -isystem /home/oriz_1st/anaconda3/envs/tensorflow/include -fPIC -I/home/oriz_1st/.local/lib/python3.9/site-packages/numpy/core/include -I../common -I/home/oriz_1st/anaconda3/envs/tensorflow/include/python3.9 -c ../common/maskApi.c -o build/temp.linux-x86_64-3.9/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "   46 |       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "      |       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   46 |       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
      "      |                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  166 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "  166 |   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
      "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  167 |   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "      |   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "  167 |   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
      "      |                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  212 |       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "      |       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "  212 |       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
      "      |                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  220 |   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
      "  220 |   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
      "      |                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmisleading-indentation\u0007-Wmisleading-indentation\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  228 |     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "      |     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "  228 |     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
      "      |                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "gcc -pthread -B /home/oriz_1st/anaconda3/envs/tensorflow/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/oriz_1st/anaconda3/envs/tensorflow/include -I/home/oriz_1st/anaconda3/envs/tensorflow/include -fPIC -O2 -isystem /home/oriz_1st/anaconda3/envs/tensorflow/include -fPIC -I/home/oriz_1st/.local/lib/python3.9/site-packages/numpy/core/include -I../common -I/home/oriz_1st/anaconda3/envs/tensorflow/include/python3.9 -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.9/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "creating build/lib.linux-x86_64-3.9\n",
      "creating build/lib.linux-x86_64-3.9/pycocotools\n",
      "gcc -pthread -B /home/oriz_1st/anaconda3/envs/tensorflow/compiler_compat -shared -Wl,-rpath,/home/oriz_1st/anaconda3/envs/tensorflow/lib -Wl,-rpath-link,/home/oriz_1st/anaconda3/envs/tensorflow/lib -L/home/oriz_1st/anaconda3/envs/tensorflow/lib -L/home/oriz_1st/anaconda3/envs/tensorflow/lib -Wl,-rpath,/home/oriz_1st/anaconda3/envs/tensorflow/lib -Wl,-rpath-link,/home/oriz_1st/anaconda3/envs/tensorflow/lib -L/home/oriz_1st/anaconda3/envs/tensorflow/lib build/temp.linux-x86_64-3.9/../common/maskApi.o build/temp.linux-x86_64-3.9/pycocotools/_mask.o -o build/lib.linux-x86_64-3.9/pycocotools/_mask.cpython-39-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-3.9/pycocotools/_mask.cpython-39-x86_64-linux-gnu.so -> pycocotools\n",
      "rm -rf build\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "A5Qr7ACCtM_b"
   },
   "outputs": [],
   "source": [
    "cp -r pycocotools models/research/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Makefile  pycocoDemo.ipynb  pycocoEvalDemo.ipynb  \u001b[0m\u001b[01;34mpycocotools\u001b[0m/  setup.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLq79dR0uQFt"
   },
   "source": [
    "### Install the Object Detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "DM2bgHvLtNFt",
    "outputId": "e23a3979-cb0f-4a8a-c834-e69f38dacaf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MZouxA5TuWgV",
    "outputId": "61f32833-47f3-468c-b299-eb0c86faace9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research\n"
     ]
    }
   ],
   "source": [
    "cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "collapsed": true,
    "id": "wvCptm6TDmT8",
    "outputId": "ba534e2c-2454-41f6-b8f8-8b56e8c2c384"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "Q635Jl58uWjI"
   },
   "outputs": [],
   "source": [
    "cp object_detection/packages/tf2/setup.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BUna9O2e88B_",
    "outputId": "1b85a7a2-b71a-438a-babc-2c4c61a0ed41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Using cached pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-21.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZyrPaXSxuWmI",
    "outputId": "e405f01f-4001-45e3-ece0-adad9d7b5d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
      "Processing /home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting avro-python3\n",
      "  Using cached avro-python3-1.10.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting apache-beam\n",
      "  Using cached apache_beam-2.34.0-cp39-cp39-linux_x86_64.whl\n",
      "Collecting pillow\n",
      "  Using cached Pillow-8.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting lxml\n",
      "  Using cached lxml-4.6.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "Requirement already satisfied: matplotlib in /home/oriz_1st/.local/lib/python3.9/site-packages (from object-detection==0.1) (3.4.3)\n",
      "Requirement already satisfied: Cython in /home/oriz_1st/.local/lib/python3.9/site-packages (from object-detection==0.1) (0.29.24)\n",
      "Collecting contextlib2\n",
      "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting tf-slim\n",
      "  Using cached tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Requirement already satisfied: six in /home/oriz_1st/.local/lib/python3.9/site-packages (from object-detection==0.1) (1.16.0)\n",
      "Requirement already satisfied: pycocotools in /home/oriz_1st/.local/lib/python3.9/site-packages (from object-detection==0.1) (2.0.2)\n",
      "Collecting lvis\n",
      "  Using cached lvis-0.5.3-py3-none-any.whl (14 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "Requirement already satisfied: pandas in /home/oriz_1st/.local/lib/python3.9/site-packages (from object-detection==0.1) (1.3.3)\n",
      "Collecting tf-models-official>=2.5.1\n",
      "  Using cached tf_models_official-2.7.0-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting tensorflow_io\n",
      "  Using cached tensorflow_io-0.22.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
      "Collecting keras==2.6.0\n",
      "  Using cached keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting gin-config\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.19.3)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Using cached kaggle-1.5.12-py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow>=2.7.0 in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
      "Collecting tensorflow-hub>=0.6.0\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting tensorflow-text>=2.7.0\n",
      "  Using cached tensorflow_text-2.7.3-cp39-cp39-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting psutil>=5.4.3\n",
      "  Using cached psutil-5.8.0-cp39-cp39-manylinux2010_x86_64.whl (293 kB)\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Using cached google_api_python_client-2.31.0-py2.py3-none-any.whl (7.8 MB)\n",
      "Collecting oauth2client\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Using cached py_cpuinfo-8.0.0-py3-none-any.whl\n",
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.5.4.60-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Using cached tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
      "Collecting tensorflow-addons\n",
      "  Using cached tensorflow_addons-0.15.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Collecting seqeval\n",
      "  Using cached seqeval-1.2.2-py3-none-any.whl\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/oriz_1st/.local/lib/python3.9/site-packages (from pandas->object-detection==0.1) (2.8.2)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tf-slim->object-detection==0.1) (0.15.0)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from apache-beam->object-detection==0.1) (3.10.0.2)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Using cached crcmod-1.7-cp39-cp39-linux_x86_64.whl\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting httplib2<0.20.0,>=0.8\n",
      "  Using cached httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
      "Collecting pyarrow<6.0.0,>=0.15.1\n",
      "  Using cached pyarrow-5.0.0-cp39-cp39-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
      "Collecting avro-python3\n",
      "  Using cached avro_python3-1.9.2.1-py3-none-any.whl\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Using cached fastavro-1.4.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from apache-beam->object-detection==0.1) (3.19.1)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Using cached pymongo-3.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (516 kB)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from apache-beam->object-detection==0.1) (1.41.1)\n",
      "Collecting future<1.0.0,>=0.18.2\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from apache-beam->object-detection==0.1) (2.26.0)\n",
      "Collecting orjson<4.0\n",
      "  Using cached orjson-3.6.4-cp39-cp39-manylinux_2_24_x86_64.whl (250 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Using cached hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting pyparsing>=2.4.0\n",
      "  Using cached pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /home/oriz_1st/.local/lib/python3.9/site-packages (from lvis->object-detection==0.1) (4.5.4.58)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from lvis->object-detection==0.1) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from lvis->object-detection==0.1) (1.3.2)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from pycocotools->object-detection==0.1) (59.2.0)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.22.0\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.22.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Collecting google-api-core<3.0.0dev,>=1.21.0\n",
      "  Using cached google_api_core-2.2.2-py2.py3-none-any.whl (95 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.3.3)\n",
      "Collecting uritemplate<5,>=3.0.0\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting docopt\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting pyparsing>=2.4.0\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting python-slugify\n",
      "  Using cached python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Requirement already satisfied: tqdm in /home/oriz_1st/.local/lib/python3.9/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.62.3)\n",
      "Requirement already satisfied: urllib3 in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.26.7)\n",
      "Requirement already satisfied: certifi in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.10.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/oriz_1st/.local/lib/python3.9/site-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/oriz_1st/.local/lib/python3.9/site-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/oriz_1st/.local/lib/python3.9/site-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.0.8)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (1.11.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (2.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (3.5.0)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "INFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "INFO: pip is looking at multiple versions of python-dateutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "INFO: pip is looking at multiple versions of pyparsing to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyparsing>=2.4.0\n",
      "  Using cached pyparsing-2.4.6-py2.py3-none-any.whl (67 kB)\n",
      "INFO: pip is looking at multiple versions of pymongo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Using cached pymongo-3.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (531 kB)\n",
      "INFO: pip is looking at multiple versions of pydot to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Using cached pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "INFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyarrow<6.0.0,>=0.15.1\n",
      "  Using cached pyarrow-4.0.1-cp39-cp39-manylinux2014_x86_64.whl (21.9 MB)\n",
      "INFO: pip is looking at multiple versions of py-cpuinfo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Using cached py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of psutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting psutil>=5.4.3\n",
      "  Using cached psutil-5.7.3.tar.gz (465 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf<4,>=3.12.2\n",
      "  Using cached protobuf-3.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "INFO: pip is looking at multiple versions of orjson to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting orjson<4.0\n",
      "  Using cached orjson-3.6.3-cp39-cp39-manylinux_2_24_x86_64.whl (234 kB)\n",
      "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python>=4.1.0.25\n",
      "  Using cached opencv_python-4.5.4.60-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
      "INFO: pip is looking at multiple versions of oauth2client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting oauth2client\n",
      "  Using cached oauth2client-4.1.2-py2.py3-none-any.whl (99 kB)\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy>=1.15.4\n",
      "  Using cached numpy-1.20.3-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.4 MB)\n",
      "INFO: pip is looking at multiple versions of kiwisolver to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kiwisolver>=1.1.0\n",
      "  Using cached kiwisolver-1.3.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "INFO: pip is looking at multiple versions of kaggle to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kaggle>=1.3.9\n",
      "  Using cached kaggle-1.5.10.tar.gz (59 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of httplib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httplib2<0.20.0,>=0.8\n",
      "  Using cached httplib2-0.19.0-py3-none-any.whl (95 kB)\n",
      "INFO: pip is looking at multiple versions of hdfs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Using cached hdfs-2.5.8.tar.gz (41 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio<2,>=1.29.0\n",
      "  Using cached grpcio-1.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "INFO: pip is looking at multiple versions of google-api-python-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Using cached google_api_python_client-2.30.0-py2.py3-none-any.whl (7.8 MB)\n",
      "INFO: pip is looking at multiple versions of future to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of fastavro to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Using cached fastavro-1.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: pip is looking at multiple versions of dill to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of cycler to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cycler>=0.10.0\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "INFO: pip is looking at multiple versions of crcmod to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting absl-py>=0.2.2\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-io-gcs-filesystem to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of tensorflow-io to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow_io\n",
      "  Using cached tensorflow_io-0.21.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (0.21.0)\n",
      "INFO: pip is looking at multiple versions of pycocotools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.3.tar.gz (106 kB)\n",
      "     |████████████████████████████████| 106 kB 175 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of lxml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lxml\n",
      "  Using cached lxml-4.6.3-cp39-cp39-manylinux2014_x86_64.whl (6.9 MB)\n",
      "INFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pillow\n",
      "  Using cached Pillow-8.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "INFO: pip is looking at multiple versions of lvis to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lvis\n",
      "  Using cached lvis-0.5.2-py3-none-any.whl (14 kB)\n",
      "INFO: pip is looking at multiple versions of cython to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting Cython\n",
      "  Using cached Cython-0.29.24-cp39-cp39-manylinux1_x86_64.whl (1.9 MB)\n",
      "INFO: pip is looking at multiple versions of contextlib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting contextlib2\n",
      "  Using cached contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)\n",
      "INFO: pip is looking at multiple versions of avro-python3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting avro-python3\n",
      "  Using cached avro-python3-1.9.1.tar.gz (36 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of apache-beam to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting apache-beam\n",
      "  Using cached apache-beam-2.33.0.zip (2.6 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tf-slim to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.3.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "INFO: pip is looking at multiple versions of tf-models-official to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tf-models-official>=2.5.1\n",
      "  Using cached tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting tensorflow>=2.5.0\n",
      "  Using cached tensorflow-2.6.2-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "Collecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting six\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "Collecting tensorboard<2.7,>=2.6.0\n",
      "  Using cached tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting clang~=5.0\n",
      "  Using cached clang-5.0-py3-none-any.whl\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1-cp39-cp39-linux_x86_64.whl\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Using cached dm_tree-0.1.6-cp39-cp39-manylinux_2_24_x86_64.whl (94 kB)\n",
      "Collecting tensorflow-text>=2.5.0\n",
      "  Using cached tensorflow_text-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (4.9 MB)\n",
      "  Using cached tensorflow_text-2.6.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "Requirement already satisfied: colorama in /home/oriz_1st/.local/lib/python3.9/site-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.4.3)\n",
      "Collecting regex\n",
      "  Using cached regex-2021.11.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Using cached tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting portalocker\n",
      "  Using cached portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting scikit-learn>=0.21.3\n",
      "  Using cached scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "Collecting typeguard>=2.7\n",
      "  Using cached typeguard-2.13.2-py3-none-any.whl (17 kB)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.4.0-py3-none-any.whl (48 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.52.0\n",
      "  Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/oriz_1st/.local/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (0.14.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-auth<3.0.0dev,>=1.16.0\n",
      "  Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/oriz_1st/.local/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.4)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/oriz_1st/.local/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
      "Building wheels for collected packages: object-detection\n",
      "  Building wheel for object-detection (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1681358 sha256=38956441138543991cd9d4f7f895ae1ecbdac476e95a2f9a4f12d1953c3aecb3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qgcqsovl/wheels/e2/7d/6d/f71beae8a75f20958b1831895bdf6932b4d095600a8d4689da\n",
      "Successfully built object-detection\n",
      "Installing collected packages: six, google-auth, pyparsing, absl-py, wrapt, typing-extensions, threadpoolctl, text-unidecode, tensorflow-estimator, tensorboard, scipy, pillow, keras, httplib2, h5py, googleapis-common-protos, flatbuffers, clang, uritemplate, typeguard, tensorflow-metadata, tensorflow-hub, tensorflow, tabulate, scikit-learn, regex, pytz, python-slugify, promise, portalocker, google-auth-httplib2, google-api-core, future, docopt, dm-tree, dill, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-datasets, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, pydot, pyarrow, py-cpuinfo, psutil, orjson, opencv-python-headless, oauth2client, kaggle, hdfs, google-api-python-client, gin-config, fastavro, crcmod, avro-python3, tf-models-official, tensorflow-io, lxml, lvis, contextlib2, apache-beam, object-detection\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.3.3\n",
      "    Uninstalling google-auth-2.3.3:\n",
      "      Successfully uninstalled google-auth-2.3.3\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.15.0\n",
      "    Uninstalling absl-py-0.15.0:\n",
      "      Successfully uninstalled absl-py-0.15.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.7.0\n",
      "    Uninstalling keras-2.7.0:\n",
      "      Successfully uninstalled keras-2.7.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.5.0\n",
      "    Uninstalling h5py-3.5.0:\n",
      "      Successfully uninstalled h5py-3.5.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyopenssl 21.0.0 requires cryptography>=3.3, which is not installed.\n",
      "labelimg 1.8.6 requires pyqt5, which is not installed.\n",
      "bleach 4.1.0 requires webencodings, which is not installed.\n",
      "awscli 1.21.12 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "astroid 2.3.3 requires wrapt==1.11.*, but you have wrapt 1.12.1 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-1.0.0 apache-beam-2.34.0 avro-python3-1.9.2.1 clang-5.0 contextlib2-21.6.0 crcmod-1.7 dill-0.3.1.1 dm-tree-0.1.6 docopt-0.6.2 fastavro-1.4.7 flatbuffers-2.0 future-0.18.2 gin-config-0.5.0 google-api-core-2.2.2 google-api-python-client-2.31.0 google-auth-2.3.3 google-auth-httplib2-0.1.0 googleapis-common-protos-1.53.0 h5py-3.6.0 hdfs-2.6.0 httplib2-0.19.1 kaggle-1.5.12 keras-2.7.0 lvis-0.5.3 lxml-4.6.4 oauth2client-4.1.3 object-detection-0.1 opencv-python-headless-4.5.4.60 orjson-3.6.4 pillow-8.4.0 portalocker-2.3.2 promise-2.3 psutil-5.8.0 py-cpuinfo-8.0.0 pyarrow-5.0.0 pydot-1.4.2 pymongo-3.12.1 pyparsing-2.4.7 python-slugify-5.0.2 pytz-2021.3 pyyaml-6.0 regex-2021.11.10 sacrebleu-2.0.0 scikit-learn-1.0.1 scipy-1.7.3 sentencepiece-0.1.96 seqeval-1.2.2 six-1.16.0 tabulate-0.8.9 tensorboard-2.7.0 tensorflow-2.6.2 tensorflow-addons-0.15.0 tensorflow-datasets-4.4.0 tensorflow-estimator-2.7.0 tensorflow-hub-0.12.0 tensorflow-io-0.21.0 tensorflow-metadata-1.4.0 tensorflow-model-optimization-0.7.0 tensorflow-text-2.6.0 text-unidecode-1.3 tf-models-official-2.6.0 tf-slim-1.1.0 threadpoolctl-3.0.0 typeguard-2.13.2 typing-extensions-4.0.0 uritemplate-4.1.1 wrapt-1.13.3\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --use-feature=2020-resolver ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "cjlR4lsmuWpE",
    "outputId": "fb38e7a6-f962-4187-ecae-0e489c6fe5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-26 14:37:14.414687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-26 14:37:14.482827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-26 14:37:14.484217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Running tests under Python 3.9.7: /home/oriz_1st/anaconda3/envs/tensorflow/bin/python\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "2021-11-26 14:37:14.499875: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-26 14:37:14.501342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-26 14:37:14.501934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-26 14:37:14.502445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-26 14:37:14.967311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-26 14:37:14.967676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-26 14:37:14.967996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-26 14:37:14.968289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4359 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/builders/model_builder.py:1100: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(('Building experimental DeepMAC meta-arch.'\n",
      "W1126 14:37:15.199787 140578501034880 model_builder.py:1100] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 0.9s\n",
      "I1126 14:37:15.395155 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 0.9s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.46s\n",
      "I1126 14:37:15.858978 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.46s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.2s\n",
      "I1126 14:37:16.062982 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.2s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.19s\n",
      "I1126 14:37:16.249248 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.19s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.24s\n",
      "I1126 14:37:17.490406 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.24s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "I1126 14:37:17.491129 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
      "I1126 14:37:17.508009 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
      "I1126 14:37:17.518980 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
      "I1126 14:37:17.530277 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.08s\n",
      "I1126 14:37:17.605742 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.08s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.07s\n",
      "I1126 14:37:17.677941 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.07s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.08s\n",
      "I1126 14:37:17.755297 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.08s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.07s\n",
      "I1126 14:37:17.830396 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.07s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.07s\n",
      "I1126 14:37:17.905584 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.07s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.02s\n",
      "I1126 14:37:17.926244 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.02s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "I1126 14:37:18.058432 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I1126 14:37:18.058536 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
      "I1126 14:37:18.058588 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 3\n",
      "I1126 14:37:18.060193 140578501034880 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1126 14:37:18.071723 140578501034880 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1126 14:37:18.071799 140578501034880 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1126 14:37:18.114282 140578501034880 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1126 14:37:18.114381 140578501034880 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1126 14:37:18.220301 140578501034880 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1126 14:37:18.220403 140578501034880 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1126 14:37:18.329225 140578501034880 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1126 14:37:18.329326 140578501034880 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1126 14:37:18.492014 140578501034880 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1126 14:37:18.492118 140578501034880 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1126 14:37:18.656106 140578501034880 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1126 14:37:18.656207 140578501034880 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1126 14:37:18.953523 140578501034880 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1126 14:37:18.953625 140578501034880 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1126 14:37:19.004229 140578501034880 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1126 14:37:19.028017 140578501034880 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1126 14:37:19.066170 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I1126 14:37:19.066282 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 88\n",
      "I1126 14:37:19.066334 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 4\n",
      "I1126 14:37:19.067570 140578501034880 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1126 14:37:19.079031 140578501034880 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1126 14:37:19.079106 140578501034880 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1126 14:37:19.174217 140578501034880 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1126 14:37:19.174333 140578501034880 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1126 14:37:19.365706 140578501034880 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1126 14:37:19.365829 140578501034880 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1126 14:37:19.537743 140578501034880 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1126 14:37:19.537846 140578501034880 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1126 14:37:19.746927 140578501034880 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1126 14:37:19.747027 140578501034880 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1126 14:37:19.995016 140578501034880 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1126 14:37:19.995134 140578501034880 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1126 14:37:20.268993 140578501034880 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1126 14:37:20.269096 140578501034880 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1126 14:37:20.382217 140578501034880 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1126 14:37:20.406426 140578501034880 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1126 14:37:20.467649 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
      "I1126 14:37:20.467766 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 112\n",
      "I1126 14:37:20.467818 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 5\n",
      "I1126 14:37:20.469249 140578501034880 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1126 14:37:20.482461 140578501034880 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1126 14:37:20.482583 140578501034880 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1126 14:37:20.574967 140578501034880 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1126 14:37:20.575067 140578501034880 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1126 14:37:20.732495 140578501034880 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1126 14:37:20.732597 140578501034880 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1126 14:37:20.891464 140578501034880 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1126 14:37:20.891579 140578501034880 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I1126 14:37:21.136464 140578501034880 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I1126 14:37:21.136579 140578501034880 efficientnet_model.py:147] round_filter input=112 output=120\n",
      "I1126 14:37:21.352417 140578501034880 efficientnet_model.py:147] round_filter input=112 output=120\n",
      "I1126 14:37:21.352522 140578501034880 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I1126 14:37:21.736943 140578501034880 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I1126 14:37:21.737059 140578501034880 efficientnet_model.py:147] round_filter input=320 output=352\n",
      "I1126 14:37:21.848095 140578501034880 efficientnet_model.py:147] round_filter input=1280 output=1408\n",
      "I1126 14:37:21.869145 140578501034880 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1126 14:37:21.917810 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
      "I1126 14:37:21.917929 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 160\n",
      "I1126 14:37:21.917984 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 6\n",
      "I1126 14:37:21.919299 140578501034880 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I1126 14:37:21.933409 140578501034880 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I1126 14:37:21.933562 140578501034880 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1126 14:37:22.029383 140578501034880 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1126 14:37:22.029487 140578501034880 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1126 14:37:22.202752 140578501034880 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1126 14:37:22.202855 140578501034880 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1126 14:37:22.371593 140578501034880 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1126 14:37:22.371694 140578501034880 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I1126 14:37:22.640586 140578501034880 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I1126 14:37:22.640690 140578501034880 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I1126 14:37:22.923230 140578501034880 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I1126 14:37:22.923403 140578501034880 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I1126 14:37:23.292103 140578501034880 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I1126 14:37:23.292232 140578501034880 efficientnet_model.py:147] round_filter input=320 output=384\n",
      "I1126 14:37:23.406653 140578501034880 efficientnet_model.py:147] round_filter input=1280 output=1536\n",
      "I1126 14:37:23.426214 140578501034880 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1126 14:37:23.480615 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
      "I1126 14:37:23.480720 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 224\n",
      "I1126 14:37:23.480770 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 7\n",
      "I1126 14:37:23.481960 140578501034880 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1126 14:37:23.492748 140578501034880 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1126 14:37:23.492819 140578501034880 efficientnet_model.py:147] round_filter input=16 output=24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1126 14:37:23.576831 140578501034880 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1126 14:37:23.576932 140578501034880 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1126 14:37:23.815930 140578501034880 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1126 14:37:23.816052 140578501034880 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I1126 14:37:24.027268 140578501034880 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I1126 14:37:24.027370 140578501034880 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I1126 14:37:24.338751 140578501034880 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I1126 14:37:24.338853 140578501034880 efficientnet_model.py:147] round_filter input=112 output=160\n",
      "I1126 14:37:24.691030 140578501034880 efficientnet_model.py:147] round_filter input=112 output=160\n",
      "I1126 14:37:24.691154 140578501034880 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I1126 14:37:25.291865 140578501034880 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I1126 14:37:25.291968 140578501034880 efficientnet_model.py:147] round_filter input=320 output=448\n",
      "I1126 14:37:25.396697 140578501034880 efficientnet_model.py:147] round_filter input=1280 output=1792\n",
      "I1126 14:37:25.417932 140578501034880 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1126 14:37:25.481742 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
      "I1126 14:37:25.481859 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 288\n",
      "I1126 14:37:25.481909 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 7\n",
      "I1126 14:37:25.483391 140578501034880 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1126 14:37:25.499792 140578501034880 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1126 14:37:25.499922 140578501034880 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1126 14:37:25.650066 140578501034880 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1126 14:37:25.650191 140578501034880 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1126 14:37:25.940400 140578501034880 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1126 14:37:25.940509 140578501034880 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I1126 14:37:26.201346 140578501034880 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I1126 14:37:26.201451 140578501034880 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I1126 14:37:26.570280 140578501034880 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I1126 14:37:26.570384 140578501034880 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I1126 14:37:26.939508 140578501034880 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I1126 14:37:26.939610 140578501034880 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I1126 14:37:27.418576 140578501034880 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I1126 14:37:27.418678 140578501034880 efficientnet_model.py:147] round_filter input=320 output=512\n",
      "I1126 14:37:27.573632 140578501034880 efficientnet_model.py:147] round_filter input=1280 output=2048\n",
      "I1126 14:37:27.592714 140578501034880 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1126 14:37:27.661733 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
      "I1126 14:37:27.661837 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I1126 14:37:27.661887 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 8\n",
      "I1126 14:37:27.663080 140578501034880 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I1126 14:37:27.673790 140578501034880 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I1126 14:37:27.673881 140578501034880 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1126 14:37:27.806697 140578501034880 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1126 14:37:27.806822 140578501034880 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1126 14:37:28.326218 140578501034880 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1126 14:37:28.326322 140578501034880 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I1126 14:37:28.642152 140578501034880 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I1126 14:37:28.642255 140578501034880 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I1126 14:37:29.067010 140578501034880 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I1126 14:37:29.067111 140578501034880 efficientnet_model.py:147] round_filter input=112 output=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1126 14:37:29.486925 140578501034880 efficientnet_model.py:147] round_filter input=112 output=200\n",
      "I1126 14:37:29.487029 140578501034880 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I1126 14:37:30.070403 140578501034880 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I1126 14:37:30.070507 140578501034880 efficientnet_model.py:147] round_filter input=320 output=576\n",
      "I1126 14:37:30.225624 140578501034880 efficientnet_model.py:147] round_filter input=1280 output=2304\n",
      "I1126 14:37:30.244566 140578501034880 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1126 14:37:30.323684 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
      "I1126 14:37:30.323787 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I1126 14:37:30.323837 140578501034880 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 8\n",
      "I1126 14:37:30.325025 140578501034880 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I1126 14:37:30.336525 140578501034880 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I1126 14:37:30.336625 140578501034880 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1126 14:37:30.503756 140578501034880 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1126 14:37:30.503859 140578501034880 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I1126 14:37:30.943530 140578501034880 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I1126 14:37:30.943643 140578501034880 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I1126 14:37:31.314987 140578501034880 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I1126 14:37:31.315090 140578501034880 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I1126 14:37:31.981766 140578501034880 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I1126 14:37:31.981871 140578501034880 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I1126 14:37:32.524032 140578501034880 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I1126 14:37:32.524135 140578501034880 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I1126 14:37:33.224385 140578501034880 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I1126 14:37:33.224488 140578501034880 efficientnet_model.py:147] round_filter input=320 output=640\n",
      "I1126 14:37:33.440330 140578501034880 efficientnet_model.py:147] round_filter input=1280 output=2560\n",
      "I1126 14:37:33.459820 140578501034880 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 15.64s\n",
      "I1126 14:37:33.570580 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 15.64s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "I1126 14:37:33.579364 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "I1126 14:37:33.580804 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "I1126 14:37:33.581125 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "I1126 14:37:33.582249 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTF2Test.test_session\n",
      "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "I1126 14:37:33.583362 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "I1126 14:37:33.583693 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "I1126 14:37:33.584461 140578501034880 test_util.py:2188] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 24 tests in 19.093s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "# From within TensorFlow/models/research/\n",
    "!python object_detection/builders/model_builder_tf2_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc382MCGBuDl"
   },
   "source": [
    "### Preparing the workplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "-TxPzE3UKHlt",
    "outputId": "59eab01b-b9fb-47d3-db4f-578705260015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8CR_TDc5KXtd",
    "outputId": "d7dc9fe5-c74b-4388-c3ea-8b2f9aa3566c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'55416691_Oris Pratama_acc skripsi.pdf.ribd'\n",
      "'55416691_Oris Pratama_Agenda bimbingan.pdf.ribd'\n",
      "'Bahasa Indonesia dan Generasi Kini.gdoc'\n",
      "'Colab Notebooks'\n",
      "'Dokumentasi for OCR.gdoc'\n",
      "'File PI'\n",
      "'Form Komitmen dan Form Pertanggungjawaban PRODTS2021. [Peserta].gdoc.gdoc'\n",
      " img027.pdf\n",
      " img028.jpeg\n",
      " img028.pdf\n",
      " LPR.gslides.gslides\n",
      "'my file'\n",
      "'Oris Pratama – UG – TI – Javascript-Android Java.pdf.ribd'\n",
      " P_20201218_131950.jpg.ribd\n",
      " report.csv\n",
      " report.gsheet\n",
      "'Research LPR.gdoc.gdoc'\n",
      " Skripsi\n",
      "'Skripsi SK Teknik Informatika 2020_Lintang_edited.xlsx.ribd'\n",
      "'Standup Special Pandji'\n",
      " tensor\n",
      "'Untitled document.gdoc'\n",
      " yolov3\n",
      " yolov4\n"
     ]
    }
   ],
   "source": [
    "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
    "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
    "!ls /mydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "UMXzIKmvIuqB",
    "outputId": "80397f69-b490-4312-997a-684c0f5aa686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'tf_od_tutorial'...\n",
      "remote: Enumerating objects: 3954, done.\u001b[K\n",
      "remote: Total 3954 (delta 0), reused 0 (delta 0), pack-reused 3954\u001b[K\n",
      "Receiving objects: 100% (3954/3954), 281.91 MiB | 33.85 MiB/s, done.\n",
      "Resolving deltas: 100% (1942/1942), done.\n",
      "Checking out files: 100% (3818/3818), done.\n",
      "/content/tf_od_tutorial\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "repo_url = 'https://github.com/orizzz/tf_od_tutorial'\n",
    "%cd /content\n",
    "\n",
    "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
    "\n",
    "!git clone {repo_url}\n",
    "%cd {repo_dir_path}\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eNxbelQmJDVV",
    "outputId": "ed8ec5f3-555a-4b88-d0a1-ccdb3efb8f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/                       README.md\n",
      "\u001b[01;34mdeploy\u001b[0m/                     requirements.txt\n",
      "generate_tfrecord.py        resize_images.py\n",
      "label_map.pbtxt             spn_tensorflow_training.ipynb\n",
      "LICENSE                     tensorflow_object_detection_training_colab.ipynb\n",
      "local_inference_test.ipynb  \u001b[01;34mtest\u001b[0m/\n",
      "local_inference_test.py     xml_to_csv.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8qJbV1uauAG"
   },
   "source": [
    "### Preparing the necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/models/research'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "d2bohyWtB6Es",
    "outputId": "bff44493-db33-417d-8bed-db65d7db0c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p /mydrive/tensor/training_demo\n",
    "%cd /mydrive/tensor/training_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "T8YaBALaCITV",
    "outputId": "9b6ee546-a2db-4cf8-c933-84ccb5bce01c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "pesLUuIzCl4X",
    "outputId": "769322fd-9aa6-46e5-c199-87611dafb8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/  \u001b[01;34mexported_models\u001b[0m/  \u001b[01;34mimages\u001b[0m/  \u001b[01;34mpre-trained-models\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p annotations\n",
    "%mkdir -p exported_models\n",
    "%mkdir -p images\n",
    "%mkdir -p pre-trained-models\n",
    "%ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "6aQSRxusJg9t"
   },
   "outputs": [],
   "source": [
    "%cp -r {repo_dir_path}/data/images /content/training_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8u5NF3LoaZs5"
   },
   "outputs": [],
   "source": [
    "%cd /content/training_demo/images\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "aM7hbvyua_lV"
   },
   "outputs": [],
   "source": [
    "%cd /content/training_demo/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "P3wQwqGoaw5h",
    "outputId": "5285b372-0979-4859-bf0a-26b35c5ad303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/      generate_tfrecord.py  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/  \u001b[01;34mimages\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cp -r {repo_dir_path}/generate_tfrecord.py /mydrive/tensor/training_demo\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "A83mzmtlDi5z",
    "outputId": "7f043653-5167-436d-f158-993fe07d1ad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/      generate_tfrecord.py  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/  \u001b[01;34mimages\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cp -r {repo_dir_path}/label_map.pbtxt /mydrive/tensor/training_demo/annotations\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gL4BBRoZuWr8",
    "outputId": "736fbd0a-1352-4f2f-cbeb-34013980d5cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo/pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd /mydrive/tensor/training_demo/pre-trained-models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MItGLVY3uWu8",
    "outputId": "c5821842-e5d7-42ef-da17-11805e2fdc6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-24 06:44:53--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.157.128, 2404:6800:4008:c13::80\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.157.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 46042990 (44M) [application/x-tar]\n",
      "Saving to: ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz’\n",
      "\n",
      "ssd_mobilenet_v2_32 100%[===================>]  43.91M  19.4MB/s    in 2.3s    \n",
      "\n",
      "2021-11-24 06:44:57 (19.4 MB/s) - ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz’ saved [46042990/46042990]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tzlPcDPLuWye",
    "outputId": "62af3c61-af9f-4c3a-dcb1-b0c28607bb8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/checkpoint\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0.index\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/saved_model.pb\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "collapsed": true,
    "id": "Vf49zVcAtNJB",
    "outputId": "23514e80-8763-4dca-a9e5-4f43afe31c97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/training_demo'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MFA0L4rmyGae",
    "outputId": "a9d9c336-3d6a-4526-85cd-932873d1e124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "source": [
    "cd /mydrive/tensor/training_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dxveA1zpALrB"
   },
   "outputs": [],
   "source": [
    "%cp -r {repo_dir_path}/data/models /mydrive/tensor/training_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Pxst7lPT0Sby",
    "outputId": "f07617ef-28cd-4994-fabc-31c451494b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                          \u001b[01;34mimages\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/                                      \u001b[01;34mmodels\u001b[0m/\n",
      "faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "generate_tfrecord.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "hU2lVZfzyuar",
    "outputId": "2f8d192e-f828-4469-c695-e2acb46cccaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: annotations/train.record\n",
      "Successfully created the TFRecord file: annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "# Create train data:\n",
    "!python generate_tfrecord.py -x data/images/train -l annotations/label_map.pbtxt -o annotations/train.record\n",
    "\n",
    "# Create test data:\n",
    "!python generate_tfrecord.py -x data/images/test -l annotations/label_map.pbtxt -o annotations/test.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "collapsed": true,
    "id": "MfHABHxi56kM",
    "outputId": "10c0a8e4-80b6-4e44-a567-6756bb5ca777"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/gdrive/My Drive/tensor/training_demo'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oQO8P8xLy6bS",
    "outputId": "768e5011-d61d-455f-9b9f-e3cb278d0f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                          \u001b[01;34mimages\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/                                      \u001b[01;34mmodels\u001b[0m/\n",
      "faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "generate_tfrecord.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "-XrMHcKOVWD0",
    "outputId": "f30cb38b-d622-498d-aedc-43b886338931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                          \u001b[01;34mimages\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/                                      model_main_tf2.py\n",
      "faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz  \u001b[01;34mmodels\u001b[0m/\n",
      "generate_tfrecord.py                                  \u001b[01;34mpre-trained-models\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cp -r /content/models/research/object_detection/model_main_tf2.py /mydrive/tensor/training_demo\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JQFLVmCa4Yo"
   },
   "source": [
    "### Start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34madversarial_text\u001b[0m/    \u001b[01;34mdeep_speech\u001b[0m/                \u001b[01;34mpcl_rl\u001b[0m/\r\n",
      "\u001b[01;34mattention_ocr\u001b[0m/       \u001b[01;34mdelf\u001b[0m/                       \u001b[01;34mpycocotools\u001b[0m/\r\n",
      "\u001b[01;34maudioset\u001b[0m/            \u001b[01;34mefficient-hrl\u001b[0m/              README.md\r\n",
      "\u001b[01;34mautoaugment\u001b[0m/         \u001b[01;34mlfads\u001b[0m/                      \u001b[01;34mrebar\u001b[0m/\r\n",
      "\u001b[01;34mbuild\u001b[0m/               \u001b[01;34mlstm_object_detection\u001b[0m/      \u001b[01;34mseq_flow_lite\u001b[0m/\r\n",
      "\u001b[01;34mcocoapi\u001b[0m/             \u001b[01;34mmarco\u001b[0m/                      setup.py\r\n",
      "\u001b[01;34mcognitive_planning\u001b[0m/  \u001b[01;34mnst_blogpost\u001b[0m/               \u001b[01;34mslim\u001b[0m/\r\n",
      "\u001b[01;34mcvt_text\u001b[0m/            \u001b[01;34mobject_detection\u001b[0m/           \u001b[01;34mvid2depth\u001b[0m/\r\n",
      "\u001b[01;34mdeeplab\u001b[0m/             \u001b[01;34mobject_detection.egg-info\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uO_IHATIEqV1",
    "outputId": "7a85ed62-fc90-4d9d-a7d2-897af851b8d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/training_demo\n"
     ]
    }
   ],
   "source": [
    "cd training_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNVwlSCq9pr1",
    "outputId": "209122fb-9ff2-439f-ab03-2f45a1419851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 14:54:36.051983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:54:36.057424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:54:36.057744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:54:36.058329: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 14:54:36.059064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:54:36.059379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:54:36.059672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:54:36.496841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:54:36.497189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:54:36.497476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:54:36.497738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4253 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "I1129 14:54:36.555391 139931663913856 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\n",
      "I1129 14:54:36.558245 139931663913856 config_util.py:552] Maybe overwriting train_steps: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1129 14:54:36.558319 139931663913856 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W1129 14:54:36.585856 139931663913856 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']\n",
      "I1129 14:54:36.588220 139931663913856 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']\n",
      "I1129 14:54:36.588326 139931663913856 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1129 14:54:36.588379 139931663913856 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1129 14:54:36.588424 139931663913856 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "W1129 14:54:36.590182 139931663913856 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1129 14:54:36.670346 139931663913856 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1129 14:54:41.698313 139931663913856 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W1129 14:54:44.183361 139931663913856 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1129 14:54:46.256031 139931663913856 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 14:54:47.966618: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "/home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:54:53.365560 139917602575936 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:54:53.365738 139917602575936 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:54:53.365837 139917602575936 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:54:53.365926 139917602575936 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:54:53.366011 139917602575936 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:54:53.366094 139917602575936 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "2021-11-29 14:54:59.210442: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W1129 14:55:02.069324 139931663913856 util.py:203] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W1129 14:55:02.069459 139931663913856 util.py:203] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "W1129 14:55:02.069512 139931663913856 util.py:203] Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "W1129 14:55:02.069617 139931663913856 util.py:211] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "!python model_main_tf2.py --model_dir=models/spn_ssd_mobilenet_v2_320x320 --pipeline_config_path=models/spn_ssd_mobilenet_v2_320x320/pipeline.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vT8TkYh-OISw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 14:30:35.304335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:30:35.309716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:30:35.310045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
      "W1129 14:30:35.312730 140157762728832 model_lib_v2.py:1081] Forced number of epochs for all eval validations to be 1.\n",
      "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\n",
      "I1129 14:30:35.312879 140157762728832 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1129 14:30:35.312956 140157762728832 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
      "I1129 14:30:35.313018 140157762728832 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
      "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "W1129 14:30:35.313090 140157762728832 model_lib_v2.py:1099] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "2021-11-29 14:30:35.315449: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 14:30:35.316179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:30:35.316561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:30:35.316859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:30:35.754255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:30:35.754610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:30:35.754897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 14:30:35.755156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4120 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "INFO:tensorflow:Reading unweighted datasets: ['annotations/test.record']\n",
      "I1129 14:30:35.833309 140157762728832 dataset_builder.py:163] Reading unweighted datasets: ['annotations/test.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['annotations/test.record']\n",
      "I1129 14:30:35.833444 140157762728832 dataset_builder.py:80] Reading record datasets for input file: ['annotations/test.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1129 14:30:35.833498 140157762728832 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1129 14:30:35.833548 140157762728832 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "W1129 14:30:35.834764 140157762728832 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1129 14:30:35.917963 140157762728832 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1129 14:30:38.943276 140157762728832 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1129 14:30:39.674994 140157762728832 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Waiting for new checkpoint at models/spn_ssd_mobilenet_v2_320x320\n",
      "I1129 14:30:41.616720 140157762728832 checkpoint_utils.py:140] Waiting for new checkpoint at models/spn_ssd_mobilenet_v2_320x320\n",
      "INFO:tensorflow:Found new checkpoint at models/spn_ssd_mobilenet_v2_320x320/ckpt-16\n",
      "I1129 14:30:41.617636 140157762728832 checkpoint_utils.py:149] Found new checkpoint at models/spn_ssd_mobilenet_v2_320x320/ckpt-16\n",
      "/home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 14:30:41.715823: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:30:46.532271 140157762728832 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:30:46.532495 140157762728832 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:30:46.532631 140157762728832 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:30:46.532755 140157762728832 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:30:46.532876 140157762728832 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 14:30:46.532995 140157762728832 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "2021-11-29 14:30:56.930944: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1129 14:30:58.425944 140157762728832 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Finished eval step 0\n",
      "I1129 14:30:58.431982 140157762728832 model_lib_v2.py:958] Finished eval step 0\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:464: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W1129 14:30:58.546383 140157762728832 deprecation.py:339] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:464: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "INFO:tensorflow:Finished eval step 100\n",
      "I1129 14:31:02.214227 140157762728832 model_lib_v2.py:958] Finished eval step 100\n",
      "INFO:tensorflow:Performing evaluation on 190 images.\n",
      "I1129 14:31:04.120373 140157762728832 coco_evaluation.py:293] Performing evaluation on 190 images.\n",
      "creating index...\n",
      "index created!\n",
      "INFO:tensorflow:Loading and preparing annotation results...\n",
      "I1129 14:31:04.120977 140157762728832 coco_tools.py:116] Loading and preparing annotation results...\n",
      "INFO:tensorflow:DONE (t=0.01s)\n",
      "I1129 14:31:04.128577 140157762728832 coco_tools.py:138] DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.55s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.12s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.093\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.049\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.140\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.187\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.015\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370\n",
      "INFO:tensorflow:Eval metrics at step 15000\n",
      "I1129 14:31:04.815951 140157762728832 model_lib_v2.py:1007] Eval metrics at step 15000\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP: 0.035741\n",
      "I1129 14:31:04.818743 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP: 0.035741\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP@.50IOU: 0.093340\n",
      "I1129 14:31:04.819755 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP@.50IOU: 0.093340\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP@.75IOU: 0.022560\n",
      "I1129 14:31:04.820621 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP@.75IOU: 0.022560\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (small): 0.000000\n",
      "I1129 14:31:04.821474 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP (small): 0.000000\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (medium): 0.000068\n",
      "I1129 14:31:04.822249 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP (medium): 0.000068\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (large): 0.049473\n",
      "I1129 14:31:04.822981 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Precision/mAP (large): 0.049473\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@1: 0.139591\n",
      "I1129 14:31:04.823755 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@1: 0.139591\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@10: 0.186613\n",
      "I1129 14:31:04.824511 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@10: 0.186613\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100: 0.300350\n",
      "I1129 14:31:04.825266 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@100: 0.300350\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (small): 0.000000\n",
      "I1129 14:31:04.825921 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@100 (small): 0.000000\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (medium): 0.014706\n",
      "I1129 14:31:04.826645 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@100 (medium): 0.014706\n",
      "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (large): 0.370265\n",
      "I1129 14:31:04.827277 140157762728832 model_lib_v2.py:1010] \t+ DetectionBoxes_Recall/AR@100 (large): 0.370265\n",
      "INFO:tensorflow:\t+ Loss/localization_loss: 0.507364\n",
      "I1129 14:31:04.827795 140157762728832 model_lib_v2.py:1010] \t+ Loss/localization_loss: 0.507364\n",
      "INFO:tensorflow:\t+ Loss/classification_loss: 322.105835\n",
      "I1129 14:31:04.828310 140157762728832 model_lib_v2.py:1010] \t+ Loss/classification_loss: 322.105835\n",
      "INFO:tensorflow:\t+ Loss/regularization_loss: 172265.093750\n",
      "I1129 14:31:04.828813 140157762728832 model_lib_v2.py:1010] \t+ Loss/regularization_loss: 172265.093750\n",
      "INFO:tensorflow:\t+ Loss/total_loss: 172587.718750\n",
      "I1129 14:31:04.829312 140157762728832 model_lib_v2.py:1010] \t+ Loss/total_loss: 172587.718750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Waiting for new checkpoint at models/spn_ssd_mobilenet_v2_320x320\n",
      "I1129 14:35:41.714750 140157762728832 checkpoint_utils.py:140] Waiting for new checkpoint at models/spn_ssd_mobilenet_v2_320x320\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/training_demo/model_main_tf2.py\", line 115, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/absl/app.py\", line 303, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/training_demo/model_main_tf2.py\", line 82, in main\n",
      "    model_lib_v2.eval_continuously(\n",
      "  File \"/home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/object_detection/model_lib_v2.py\", line 1128, in eval_continuously\n",
      "    for latest_checkpoint in tf.train.checkpoints_iterator(\n",
      "  File \"/home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 198, in checkpoints_iterator\n",
      "    new_checkpoint_path = wait_for_new_checkpoint(\n",
      "  File \"/home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 147, in wait_for_new_checkpoint\n",
      "    time.sleep(seconds_to_sleep)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python model_main_tf2.py --model_dir=models/spn_ssd_mobilenet_v2_320x320 --pipeline_config_path=models/spn_ssd_mobilenet_v2_320x320/pipeline.config --checkpoint_dir=models/spn_ssd_mobilenet_v2_320x320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": true,
    "id": "TlRqjT4AFOhv",
    "outputId": "a77528bd-f60a-4846-cf1a-f2b7eb8d3513"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/training_demo'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeI2URnR9zhw",
    "outputId": "2defe90f-c72b-49e4-a7c3-e8d6cdf145b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 15:19:31.398504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:19:31.404090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:19:31.404447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:19:31.411207: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 15:19:31.411933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:19:31.412395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:19:31.412773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:19:31.850032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:19:31.850370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:19:31.850671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:19:31.850931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4214 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "WARNING:tensorflow:From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "W1129 15:19:32.015932 140332396663680 deprecation.py:611] From /home/oriz_1st/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:19:35.992109 140332396663680 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:19:35.992328 140332396663680 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:19:35.992466 140332396663680 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:19:35.992595 140332396663680 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:19:35.992720 140332396663680 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:19:35.992845 140332396663680 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7fa0dc37b220>, because it is not built.\n",
      "W1129 15:19:42.475661 140332396663680 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7fa0dc37b220>, because it is not built.\n",
      "2021-11-29 15:19:49.854697: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "W1129 15:20:02.020085 140332396663680 save.py:249] Found untraced functions such as BoxPredictor_layer_call_and_return_conditional_losses, BoxPredictor_layer_call_fn, BoxPredictor_layer_call_fn, BoxPredictor_layer_call_and_return_conditional_losses, BoxPredictor_layer_call_and_return_conditional_losses while saving (showing 5 of 125). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: exported_models/spn_detection_model/saved_model/assets\n",
      "I1129 15:20:05.492643 140332396663680 builder_impl.py:780] Assets written to: exported_models/spn_detection_model/saved_model/assets\n",
      "INFO:tensorflow:Writing pipeline config file to exported_models/spn_detection_model/pipeline.config\n",
      "I1129 15:20:06.116218 140332396663680 config_util.py:253] Writing pipeline config file to exported_models/spn_detection_model/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "!python exporter_main_v2.py --input_type image_tensor --pipeline_config_path models/spn_ssd_mobilenet_v2_320x320/pipeline.config --trained_checkpoint_dir models/spn_ssd_mobilenet_v2_320x320 --output_directory exported_models/spn_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/  tensorflow_training.ipynb  training_command.ipynb  \u001b[01;34mtraining_demo\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%cp -r models/research/object_detection/export_tflite_graph_tf2.py training_demo\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oriz_1st/Documents/Project/tensor-research/tf_od_local/training_demo\n"
     ]
    }
   ],
   "source": [
    "cd training_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 15:22:12.693958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:12.699411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:12.699733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:12.704832: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 15:22:12.705688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:12.706008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:12.706302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:13.147621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:13.147962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:13.148248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:13.148512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4217 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:22:16.790638 140684834808704 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:22:16.790861 140684834808704 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:22:16.790998 140684834808704 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:22:16.791128 140684834808704 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:22:16.791253 140684834808704 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1129 15:22:16.791374 140684834808704 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
      "2021-11-29 15:22:17.476169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:17.476468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:17.476675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:17.476922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:17.477130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:17.477305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4217 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2021-11-29 15:22:18.642712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:18.643037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:18.643275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:18.643546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:18.643755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:18.643928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4217 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7ff2ec364d60>, because it is not built.\n",
      "W1129 15:22:19.050410 140684834808704 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7ff2ec364d60>, because it is not built.\n",
      "2021-11-29 15:22:26.831546: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2021-11-29 15:22:27.704787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:27.705071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:27.705279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:27.705526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:27.705736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:22:27.705908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4217 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1129 15:22:38.206732 140684834808704 save.py:249] Found untraced functions such as BoxPredictor_layer_call_and_return_conditional_losses, BoxPredictor_layer_call_fn, BoxPredictor_layer_call_fn, BoxPredictor_layer_call_and_return_conditional_losses, BoxPredictor_layer_call_and_return_conditional_losses while saving (showing 5 of 125). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: exported_models/spn_detection_model/saved_model/assets\n",
      "I1129 15:22:41.338922 140684834808704 builder_impl.py:780] Assets written to: exported_models/spn_detection_model/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "!python export_tflite_graph_tf2.py  --pipeline_config_path=models/spn_ssd_mobilenet_v2_320x320/pipeline.config --trained_checkpoint_dir=models/spn_ssd_mobilenet_v2_320x320 --output_directory=exported_models/spn_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 15:29:43.642472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:29:43.648001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:29:43.648326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:29:44.091502: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 15:29:44.092068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:29:44.092421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:29:44.092714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:29:44.561133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:29:44.561457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:29:44.561739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 15:29:44.562003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4266 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2021-11-29 15:29:53.418151: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-11-29 15:29:53.418180: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2021-11-29 15:29:53.418186: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored change_concat_input_ranges.\n",
      "2021-11-29 15:29:53.419004: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: exported_models/spn_detection_model/saved_model\n",
      "2021-11-29 15:29:53.499880: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-11-29 15:29:53.499917: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: exported_models/spn_detection_model/saved_model\n",
      "2021-11-29 15:29:53.776942: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\n",
      "2021-11-29 15:29:54.313460: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: exported_models/spn_detection_model/saved_model\n",
      "2021-11-29 15:29:54.529101: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 1110097 microseconds.\n",
      "2021-11-29 15:29:55.275113: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2021-11-29 15:29:55.939913: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1899] Estimated count of arithmetic ops: 1.296 G  ops, equivalently 0.648 G  MACs\n"
     ]
    }
   ],
   "source": [
    "!tflite_convert --saved_model_dir=\"exported_models/spn_detection_model/saved_model\" --output_file=\"exported_models/spn_detection_model/saved_model/detect.tflite\" --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --allow_custom_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIIKwpxtGAhK"
   },
   "source": [
    "Inferencing My Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "spZJ4ms3FqRT"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Object Detection (On Image) From TF2 Saved Model\n",
    "=====================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import argparse\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# PROVIDE PATH TO IMAGE DIRECTORY\n",
    "IMAGE_PATHS = '/content/training_demo/images/train/image1.jpg'\n",
    "\n",
    "\n",
    "# PROVIDE PATH TO MODEL DIRECTORY\n",
    "PATH_TO_MODEL_DIR = '/content/training_demo/exported_models/my_model'\n",
    "\n",
    "# PROVIDE PATH TO LABEL MAP\n",
    "PATH_TO_LABELS = '/content/training_demo/annotations/label_map.pbtxt'\n",
    "\n",
    "# PROVIDE THE MINIMUM CONFIDENCE THRESHOLD\n",
    "MIN_CONF_THRESH = float(0.60)\n",
    "\n",
    "# LOAD THE MODEL\n",
    "\n",
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))\n",
    "\n",
    "# LOAD LABEL MAP DATA FOR PLOTTING\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Running inference for {}... '.format(IMAGE_PATHS), end='')\n",
    "\n",
    "image = cv2.imread(IMAGE_PATHS)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_expanded = np.expand_dims(image_rgb, axis=0)\n",
    "\n",
    "# The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "input_tensor = tf.convert_to_tensor(image)\n",
    "# The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "# input_tensor = np.expand_dims(image_np, 0)\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "# All outputs are batches tensors.\n",
    "# Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "# We're only interested in the first num_detections.\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "               for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "image_with_detections = image.copy()\n",
    "\n",
    "# SET MIN_SCORE_THRESH BASED ON YOU MINIMUM THRESHOLD FOR DETECTIONS\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_with_detections,\n",
    "      detections['detection_boxes'],\n",
    "      detections['detection_classes'],\n",
    "      detections['detection_scores'],\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=0.5,\n",
    "      agnostic_mode=False)\n",
    "\n",
    "print('Done')\n",
    "# DISPLAYS OUTPUT IMAGE\n",
    "cv2_imshow(image_with_detections)\n",
    "# CLOSES WINDOW ONCE KEY IS PRESSED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "k0KheEfPGYhO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of spn tensorflow training di akun spn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
