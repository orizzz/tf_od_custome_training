{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GnVG7OmvsAOR",
    "outputId": "53d6d057-7af6-4b30-e43a-fba64f321ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 489.6 MB 24 kB/s \n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 45.8 MB/s \n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.42.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 32.6 MB/s \n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.4 MB 15.3 MB/s \n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.22.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 34.7 MB/s \n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 49.2 MB/s \n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting wheel<1.0,>=0.32.0\n",
      "  Using cached wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "\u001b[K     |████████████████████████████████| 463 kB 48.9 MB/s \n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 53.7 MB/s \n",
      "\u001b[?25hCollecting gast<0.5.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
      "\u001b[?25hCollecting numpy>=1.14.5\n",
      "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 37.0 MB/s \n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Using cached setuptools-59.2.0-py3-none-any.whl (952 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, setuptools, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
      "tensorflow-metadata 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 1.0.0 which is incompatible.\n",
      "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
      "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "google-api-core 1.26.3 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.3.3 which is incompatible.\n",
      "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.2 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.0 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.7 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.8.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 numpy-1.21.4 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-59.2.0 six-1.16.0 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0 typing-extensions-4.0.0 urllib3-1.26.7 werkzeug-2.0.2 wheel-0.37.0 wrapt-1.13.3 zipp-3.6.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "absl",
         "astunparse",
         "certifi",
         "charset_normalizer",
         "gast",
         "google",
         "h5py",
         "idna",
         "numpy",
         "opt_einsum",
         "pkg_resources",
         "pyasn1",
         "pyasn1_modules",
         "requests",
         "rsa",
         "six",
         "termcolor",
         "urllib3",
         "wrapt"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tensorflow --ignore-installed --upgrade \n",
    "#tensorflow-gpu==2.4.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeyP0aYes9Zs",
    "outputId": "446538fc-01d0-4b95-c156-0347649feb24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow_version : 2.7.0\n",
      "cuda_version : 11.2\n",
      "cudnn_version : 8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"tensorflow_version :\",tf.__version__)\n",
    "\n",
    "sys_details = tf.sysconfig.get_build_info()\n",
    "cuda_version = sys_details[\"cuda_version\"]\n",
    "print(\"cuda_version :\",cuda_version)\n",
    "  \n",
    "cudnn_version = sys_details[\"cudnn_version\"]  \n",
    "print(\"cudnn_version :\", cudnn_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8AOOLDGAsPW",
    "outputId": "0e957e32-a99b-4bba-80a9-9490005ef83e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
      "Cuda compilation tools, release 11.1, V11.1.105\n",
      "Build cuda_11.1.TC455_06.29190527_0\n",
      "Wed Nov 24 04:45:06 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEGpXLl3tQVX"
   },
   "source": [
    "## **Cloning TFOD 2.0 Github**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFkdXoEltLY9",
    "outputId": "7eb1d02e-d54f-4ebc-9ad7-da18d9cf6fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 67412, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 67412 (delta 0), reused 1 (delta 0), pack-reused 67405\u001b[K\n",
      "Receiving objects: 100% (67412/67412), 576.24 MiB | 28.90 MiB/s, done.\n",
      "Resolving deltas: 100% (47315/47315), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aJ4YzpQ4tMlz",
    "outputId": "e0f424d7-938e-4707-c905-7d3d17b7aec5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZwI0sTdtMsc",
    "outputId": "991074c3-5878-41d2-bba0-98c65e07cdb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/models/research\n"
     ]
    }
   ],
   "source": [
    "cd /content/models/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "SK5RyNz9tMvZ",
    "outputId": "436d4afe-bdfc-4c26-b61d-552adb5db5fb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/models/research'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NozH3MfAtMyR"
   },
   "outputs": [],
   "source": [
    "!protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alOqL7ortM1F",
    "outputId": "c872534d-2d0d-4bb4-fbdf-98d009ea6699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cocoapi'...\n",
      "remote: Enumerating objects: 975, done.\u001b[K\n",
      "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
      "Receiving objects: 100% (975/975), 11.72 MiB | 21.10 MiB/s, done.\n",
      "Resolving deltas: 100% (576/576), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cocodataset/cocoapi.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XRlMiuEtM4R",
    "outputId": "07c01d9b-d900-46de-9cb5-a5d7f51fe803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/models/research/cocoapi/PythonAPI\n"
     ]
    }
   ],
   "source": [
    "cd cocoapi/PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZ-xj6MUtM7U",
    "outputId": "daad3ed7-9a5e-4d54-9299-5d324fb1d954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python setup.py build_ext --inplace\n",
      "running build_ext\n",
      "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
      "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/models/research/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "building 'pycocotools._mask' extension\n",
      "creating build\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-3.7\n",
      "creating build/temp.linux-x86_64-3.7/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
      "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
      "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
      "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
      "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
      "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
      "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
      "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
      "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
      "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "creating build/lib.linux-x86_64-3.7\n",
      "creating build/lib.linux-x86_64-3.7/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/../common/maskApi.o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -o build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so -> pycocotools\n",
      "rm -rf build\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "A5Qr7ACCtM_b"
   },
   "outputs": [],
   "source": [
    "cp -r pycocotools /content/models/research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLq79dR0uQFt"
   },
   "source": [
    "### Install the Object Detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM2bgHvLtNFt",
    "outputId": "e23a3979-cb0f-4a8a-c834-e69f38dacaf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/models/research/cocoapi\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZouxA5TuWgV",
    "outputId": "61f32833-47f3-468c-b299-eb0c86faace9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/models/research\n"
     ]
    }
   ],
   "source": [
    "cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wvCptm6TDmT8",
    "outputId": "ba534e2c-2454-41f6-b8f8-8b56e8c2c384"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/models/research'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Q635Jl58uWjI"
   },
   "outputs": [],
   "source": [
    "cp object_detection/packages/tf2/setup.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUna9O2e88B_",
    "outputId": "1b85a7a2-b71a-438a-babc-2c4c61a0ed41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
      "Collecting pip\n",
      "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.3\n",
      "    Uninstalling pip-21.1.3:\n",
      "      Successfully uninstalled pip-21.1.3\n",
      "Successfully installed pip-21.3.1\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyrPaXSxuWmI",
    "outputId": "e405f01f-4001-45e3-ece0-adad9d7b5d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
      "Processing /content/models/research\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting apache-beam\n",
      "  Downloading apache_beam-2.34.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
      "     |████████████████████████████████| 9.8 MB 8.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.24)\n",
      "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
      "Collecting tf-slim\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "     |████████████████████████████████| 352 kB 48.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.16.0)\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.2)\n",
      "Collecting lvis\n",
      "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)\n",
      "Collecting tf-models-official>=2.5.1\n",
      "  Downloading tf_models_official-2.7.0-py2.py3-none-any.whl (1.8 MB)\n",
      "     |████████████████████████████████| 1.8 MB 38.9 MB/s            \n",
      "\u001b[?25hCollecting tensorflow_io\n",
      "  Downloading tensorflow_io-0.22.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
      "     |████████████████████████████████| 22.7 MB 1.7 MB/s             \n",
      "\u001b[?25hCollecting keras==2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 37.9 MB/s            \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 37.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
      "     |████████████████████████████████| 47.6 MB 1.9 MB/s             \n",
      "\u001b[?25hCollecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 36.7 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
      "     |████████████████████████████████| 213 kB 43.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     |████████████████████████████████| 43 kB 1.8 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "     |████████████████████████████████| 99 kB 7.8 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "     |████████████████████████████████| 90 kB 7.5 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n",
      "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.21.4)\n",
      "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
      "Requirement already satisfied: tensorflow>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "     |████████████████████████████████| 596 kB 43.5 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-text>=2.7.0\n",
      "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 35.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.8)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->object-detection==0.1) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim->object-detection==0.1) (1.0.0)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "     |████████████████████████████████| 151 kB 48.1 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (2.26.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
      "Collecting numpy>=1.15.4\n",
      "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
      "     |████████████████████████████████| 15.3 MB 57.7 MB/s            \n",
      "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.42.0)\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Downloading fastavro-1.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "     |████████████████████████████████| 2.3 MB 36.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyarrow<6.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n",
      "Collecting typing-extensions<4,>=3.7.0\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.12.1)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
      "Collecting future<1.0.0,>=0.18.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "     |████████████████████████████████| 829 kB 42.4 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.19.1)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB)\n",
      "     |████████████████████████████████| 249 kB 47.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (3.0.6)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.3.2)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->object-detection==0.1) (59.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.22.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io->object-detection==0.1) (0.22.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.26.3)\n",
      "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.3.3)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.62.3)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.26.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.10.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.0.7)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (3.6.0)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "     |████████████████████████████████| 636 kB 45.9 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "     |████████████████████████████████| 503 kB 44.0 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of python-dateutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     |████████████████████████████████| 247 kB 48.6 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pyparsing to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyparsing>=2.4.0\n",
      "  Downloading pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 6.2 MB/s             \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pymongo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "     |████████████████████████████████| 507 kB 39.8 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pydot to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "INFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyarrow<6.0.0,>=0.15.1\n",
      "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "     |████████████████████████████████| 23.6 MB 1.5 MB/s             \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of py-cpuinfo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
      "     |████████████████████████████████| 95 kB 4.3 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "INFO: pip is looking at multiple versions of psutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting psutil>=5.4.3\n",
      "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
      "     |████████████████████████████████| 296 kB 48.3 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf<4,>=3.12.2\n",
      "  Using cached protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "INFO: pip is looking at multiple versions of orjson to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.6.3-cp37-cp37m-manylinux_2_24_x86_64.whl (234 kB)\n",
      "     |████████████████████████████████| 234 kB 48.9 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python>=4.1.0.25\n",
      "  Downloading opencv_python-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
      "     |████████████████████████████████| 60.3 MB 1.6 MB/s             \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of oauth2client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "     |████████████████████████████████| 98 kB 7.2 MB/s             \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy>=1.15.4\n",
      "  Downloading numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "     |████████████████████████████████| 15.3 MB 20.7 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of kiwisolver to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kiwisolver>=1.1.0\n",
      "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 39.1 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of kaggle to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "     |████████████████████████████████| 58 kB 4.4 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "INFO: pip is looking at multiple versions of httplib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httplib2<0.20.0,>=0.8\n",
      "  Downloading httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 4.1 MB/s             \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of hdfs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.5.8.tar.gz (41 kB)\n",
      "     |████████████████████████████████| 41 kB 666 kB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "INFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio<2,>=1.29.0\n",
      "  Using cached grpcio-1.42.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "INFO: pip is looking at multiple versions of google-api-python-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Downloading google_api_python_client-2.31.0-py2.py3-none-any.whl (7.8 MB)\n",
      "     |████████████████████████████████| 7.8 MB 37.4 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of future to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of fastavro to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Downloading fastavro-1.4.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "     |████████████████████████████████| 2.3 MB 19.5 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of dill to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of cycler to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cycler>=0.10.0\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "INFO: pip is looking at multiple versions of crcmod to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "     |████████████████████████████████| 89 kB 8.4 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "INFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting absl-py>=0.2.2\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-io-gcs-filesystem to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-io-gcs-filesystem==0.22.0\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.22.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-io to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow_io\n",
      "  Downloading tensorflow_io-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
      "     |████████████████████████████████| 22.7 MB 2.4 MB/s             \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pycocotools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "INFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pillow\n",
      "  Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 35.3 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of lxml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB)\n",
      "     |████████████████████████████████| 6.3 MB 38.7 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "     |████████████████████████████████| 11.2 MB 13.9 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of lvis to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lvis\n",
      "  Downloading lvis-0.5.2-py3-none-any.whl (14 kB)\n",
      "INFO: pip is looking at multiple versions of cython to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.24-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 27.7 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of contextlib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting contextlib2\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "INFO: pip is looking at multiple versions of avro-python3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.9.1.tar.gz (36 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "INFO: pip is looking at multiple versions of apache-beam to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting apache-beam\n",
      "  Downloading apache_beam-2.33.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
      "     |████████████████████████████████| 9.8 MB 22.7 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tf-slim to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.2 MB)\n",
      "     |████████████████████████████████| 38.2 MB 1.4 MB/s             \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     |████████████████████████████████| 11.3 MB 38.3 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tf-models-official to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tf-models-official>=2.5.1\n",
      "  Downloading tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
      "     |████████████████████████████████| 1.8 MB 25.2 MB/s            \n",
      "\u001b[?25hCollecting tensorflow>=2.5.0\n",
      "  Downloading tensorflow-2.6.2-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "     |████████████████████████████████| 458.3 MB 12 kB/s              \n",
      "\u001b[?25hCollecting six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 40.1 MB/s            \n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tensorboard<2.7,>=2.6.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "     |████████████████████████████████| 5.6 MB 23.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n",
      "Collecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "     |████████████████████████████████| 462 kB 46.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
      "Collecting numpy>=1.15.4\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "     |████████████████████████████████| 14.8 MB 14.3 MB/s            \n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "     |████████████████████████████████| 132 kB 40.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 42.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.6)\n",
      "Collecting tensorflow-text>=2.5.0\n",
      "  Downloading tensorflow_text-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 21.7 MB/s            \n",
      "\u001b[?25h  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
      "     |████████████████████████████████| 4.4 MB 37.9 MB/s            \n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.4.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.4.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
      "Collecting google-auth>=1.16.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |████████████████████████████████| 152 kB 47.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.53.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.6.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "     |████████████████████████████████| 129 kB 49.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (4.8.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
      "Building wheels for collected packages: object-detection, avro-python3, dill, future, py-cpuinfo, seqeval, clang, wrapt\n",
      "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=21901864 sha256=5646f26a7516bf927426182447ace34f2921d1483ea3782638d43234c5a85715\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4ru3w7zh/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
      "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=47f699bb741b7f078fe7d7cffdfff27b8f4bc71642649ed6ae6ae40fc50dd9fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=53a4c97225b24f845297f4b760cf4ebcf03d8fa9b0a0cde697ef9ea163de116f\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=254a5ff73bef9fac878cf9dd7c41359070ce7aaceab3ba74d6e6ae5ca5b3cf73\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=5be005732de7b20221f5309e2efe6b6cd9991302214a8442c57e3ddffa13ed50\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=5dbcb6ca2aac26b59c2b3683908c322e3d407ab0134dc490efaeb7fca537bb78\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=9e3fe76bbb2a45861b779fc06432cdc6bad9a27545b6b61f2b9d707c9207fa41\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68721 sha256=b7758ae60471bd346125bb9055e5be955085ce4ef1470fc00a18f941dd16729f\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built object-detection avro-python3 dill future py-cpuinfo seqeval clang wrapt\n",
      "Installing collected packages: typing-extensions, six, google-auth, numpy, absl-py, wrapt, tensorflow-estimator, tensorboard, keras, h5py, flatbuffers, clang, tensorflow, portalocker, future, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-io-gcs-filesystem, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, orjson, opencv-python-headless, hdfs, fastavro, avro-python3, tf-models-official, tensorflow-io, lvis, apache-beam, object-detection\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.0\n",
      "    Uninstalling typing-extensions-4.0.0:\n",
      "      Successfully uninstalled typing-extensions-4.0.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.3.3\n",
      "    Uninstalling google-auth-2.3.3:\n",
      "      Successfully uninstalled google-auth-2.3.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.4\n",
      "    Uninstalling numpy-1.21.4:\n",
      "      Successfully uninstalled numpy-1.21.4\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.0.0\n",
      "    Uninstalling absl-py-1.0.0:\n",
      "      Successfully uninstalled absl-py-1.0.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.13.3\n",
      "    Uninstalling wrapt-1.13.3:\n",
      "      Successfully uninstalled wrapt-1.13.3\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.7.0\n",
      "    Uninstalling keras-2.7.0:\n",
      "      Successfully uninstalled keras-2.7.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.6.0\n",
      "    Uninstalling h5py-3.6.0:\n",
      "      Successfully uninstalled h5py-3.6.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.22.0\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.22.0:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.22.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
      "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.12.0 apache-beam-2.34.0 avro-python3-1.9.2.1 clang-5.0 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.7 flatbuffers-2.0 future-0.18.2 google-auth-1.35.0 h5py-3.1.0 hdfs-2.6.0 keras-2.7.0 lvis-0.5.3 numpy-1.19.5 object-detection-0.1 opencv-python-headless-4.5.4.60 orjson-3.6.4 portalocker-2.3.2 py-cpuinfo-8.0.0 pyyaml-6.0 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 six-1.15.0 tensorboard-2.7.0 tensorflow-2.7.0 tensorflow-addons-0.15.0 tensorflow-estimator-2.7.0 tensorflow-io-0.21.0 tensorflow-io-gcs-filesystem-0.22.0 tensorflow-model-optimization-0.7.0 tensorflow-text-2.6.0 tf-models-official-2.6.0 tf-slim-1.1.0 typing-extensions-3.10.0.2 wrapt-1.13.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --use-feature=2020-resolver ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjlR4lsmuWpE",
    "outputId": "fb38e7a6-f962-4187-ecae-0e489c6fe5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests under Python 3.7.12: /usr/bin/python3\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "2021-11-24 04:48:34.137873: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "W1124 04:48:34.531799 140200370878336 model_builder.py:1100] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.32s\n",
      "I1124 04:48:34.866835 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.32s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.7s\n",
      "I1124 04:48:35.564441 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.7s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.46s\n",
      "I1124 04:48:36.021868 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.46s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.31s\n",
      "I1124 04:48:36.337376 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.31s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.11s\n",
      "I1124 04:48:38.443262 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.11s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "I1124 04:48:38.444382 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.03s\n",
      "I1124 04:48:38.471822 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.03s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
      "I1124 04:48:38.491093 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
      "I1124 04:48:38.509491 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.12s\n",
      "I1124 04:48:38.626368 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.12s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.27s\n",
      "I1124 04:48:38.895482 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.27s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.13s\n",
      "I1124 04:48:39.027710 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.13s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.12s\n",
      "I1124 04:48:39.148442 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.12s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.12s\n",
      "I1124 04:48:39.268170 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.12s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n",
      "I1124 04:48:39.302304 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "I1124 04:48:39.532791 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I1124 04:48:39.533048 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
      "I1124 04:48:39.533177 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 3\n",
      "I1124 04:48:39.535782 140200370878336 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 04:48:39.554980 140200370878336 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 04:48:39.555122 140200370878336 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 04:48:39.625102 140200370878336 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 04:48:39.625328 140200370878336 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 04:48:39.828090 140200370878336 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 04:48:39.828339 140200370878336 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1124 04:48:40.017398 140200370878336 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1124 04:48:40.017595 140200370878336 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1124 04:48:40.293677 140200370878336 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1124 04:48:40.293886 140200370878336 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1124 04:48:40.583826 140200370878336 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1124 04:48:40.584074 140200370878336 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1124 04:48:40.971674 140200370878336 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1124 04:48:40.971914 140200370878336 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1124 04:48:41.058925 140200370878336 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1124 04:48:41.097225 140200370878336 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 04:48:41.162893 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I1124 04:48:41.163139 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 88\n",
      "I1124 04:48:41.163287 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 4\n",
      "I1124 04:48:41.166091 140200370878336 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 04:48:41.185110 140200370878336 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 04:48:41.185261 140200370878336 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 04:48:41.334394 140200370878336 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 04:48:41.334607 140200370878336 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 04:48:41.623990 140200370878336 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 04:48:41.624229 140200370878336 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1124 04:48:41.914943 140200370878336 efficientnet_model.py:147] round_filter input=40 output=40\n",
      "I1124 04:48:41.915146 140200370878336 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1124 04:48:42.295742 140200370878336 efficientnet_model.py:147] round_filter input=80 output=80\n",
      "I1124 04:48:42.295977 140200370878336 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1124 04:48:42.789201 140200370878336 efficientnet_model.py:147] round_filter input=112 output=112\n",
      "I1124 04:48:42.789501 140200370878336 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1124 04:48:43.260774 140200370878336 efficientnet_model.py:147] round_filter input=192 output=192\n",
      "I1124 04:48:43.260996 140200370878336 efficientnet_model.py:147] round_filter input=320 output=320\n",
      "I1124 04:48:43.453302 140200370878336 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
      "I1124 04:48:43.491181 140200370878336 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 04:48:43.567711 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
      "I1124 04:48:43.567908 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 112\n",
      "I1124 04:48:43.568049 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 5\n",
      "I1124 04:48:43.569955 140200370878336 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 04:48:43.588063 140200370878336 efficientnet_model.py:147] round_filter input=32 output=32\n",
      "I1124 04:48:43.588202 140200370878336 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 04:48:43.730292 140200370878336 efficientnet_model.py:147] round_filter input=16 output=16\n",
      "I1124 04:48:43.730528 140200370878336 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 04:48:44.017490 140200370878336 efficientnet_model.py:147] round_filter input=24 output=24\n",
      "I1124 04:48:44.017719 140200370878336 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1124 04:48:44.305206 140200370878336 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1124 04:48:44.305401 140200370878336 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I1124 04:48:44.697815 140200370878336 efficientnet_model.py:147] round_filter input=80 output=88\n",
      "I1124 04:48:44.698022 140200370878336 efficientnet_model.py:147] round_filter input=112 output=120\n",
      "I1124 04:48:45.097252 140200370878336 efficientnet_model.py:147] round_filter input=112 output=120\n",
      "I1124 04:48:45.097481 140200370878336 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I1124 04:48:45.584018 140200370878336 efficientnet_model.py:147] round_filter input=192 output=208\n",
      "I1124 04:48:45.584256 140200370878336 efficientnet_model.py:147] round_filter input=320 output=352\n",
      "I1124 04:48:45.769132 140200370878336 efficientnet_model.py:147] round_filter input=1280 output=1408\n",
      "I1124 04:48:45.802932 140200370878336 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 04:48:45.880075 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
      "I1124 04:48:45.880290 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 160\n",
      "I1124 04:48:45.880404 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 6\n",
      "I1124 04:48:45.882328 140200370878336 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I1124 04:48:45.902512 140200370878336 efficientnet_model.py:147] round_filter input=32 output=40\n",
      "I1124 04:48:45.902722 140200370878336 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 04:48:46.053969 140200370878336 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 04:48:46.054165 140200370878336 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1124 04:48:46.342577 140200370878336 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1124 04:48:46.342835 140200370878336 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1124 04:48:46.627040 140200370878336 efficientnet_model.py:147] round_filter input=40 output=48\n",
      "I1124 04:48:46.627260 140200370878336 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I1124 04:48:47.124320 140200370878336 efficientnet_model.py:147] round_filter input=80 output=96\n",
      "I1124 04:48:47.124525 140200370878336 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I1124 04:48:47.838891 140200370878336 efficientnet_model.py:147] round_filter input=112 output=136\n",
      "I1124 04:48:47.839099 140200370878336 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I1124 04:48:48.412631 140200370878336 efficientnet_model.py:147] round_filter input=192 output=232\n",
      "I1124 04:48:48.412838 140200370878336 efficientnet_model.py:147] round_filter input=320 output=384\n",
      "I1124 04:48:48.595094 140200370878336 efficientnet_model.py:147] round_filter input=1280 output=1536\n",
      "I1124 04:48:48.628843 140200370878336 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 04:48:48.712784 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
      "I1124 04:48:48.713002 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 224\n",
      "I1124 04:48:48.713132 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 7\n",
      "I1124 04:48:48.715232 140200370878336 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1124 04:48:48.734424 140200370878336 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1124 04:48:48.734564 140200370878336 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 04:48:48.877770 140200370878336 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 04:48:48.877989 140200370878336 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1124 04:48:49.281266 140200370878336 efficientnet_model.py:147] round_filter input=24 output=32\n",
      "I1124 04:48:49.281461 140200370878336 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I1124 04:48:49.671287 140200370878336 efficientnet_model.py:147] round_filter input=40 output=56\n",
      "I1124 04:48:49.671486 140200370878336 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I1124 04:48:50.249782 140200370878336 efficientnet_model.py:147] round_filter input=80 output=112\n",
      "I1124 04:48:50.249989 140200370878336 efficientnet_model.py:147] round_filter input=112 output=160\n",
      "I1124 04:48:50.832258 140200370878336 efficientnet_model.py:147] round_filter input=112 output=160\n",
      "I1124 04:48:50.832478 140200370878336 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I1124 04:48:51.602359 140200370878336 efficientnet_model.py:147] round_filter input=192 output=272\n",
      "I1124 04:48:51.602597 140200370878336 efficientnet_model.py:147] round_filter input=320 output=448\n",
      "I1124 04:48:51.788033 140200370878336 efficientnet_model.py:147] round_filter input=1280 output=1792\n",
      "I1124 04:48:51.824947 140200370878336 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 04:48:51.922761 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
      "I1124 04:48:51.922978 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 288\n",
      "I1124 04:48:51.923100 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 7\n",
      "I1124 04:48:51.925188 140200370878336 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1124 04:48:51.946542 140200370878336 efficientnet_model.py:147] round_filter input=32 output=48\n",
      "I1124 04:48:51.946712 140200370878336 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 04:48:52.185229 140200370878336 efficientnet_model.py:147] round_filter input=16 output=24\n",
      "I1124 04:48:52.185450 140200370878336 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1124 04:48:52.667329 140200370878336 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1124 04:48:52.667554 140200370878336 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I1124 04:48:53.394079 140200370878336 efficientnet_model.py:147] round_filter input=40 output=64\n",
      "I1124 04:48:53.394295 140200370878336 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I1124 04:48:54.088085 140200370878336 efficientnet_model.py:147] round_filter input=80 output=128\n",
      "I1124 04:48:54.088316 140200370878336 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I1124 04:48:54.759482 140200370878336 efficientnet_model.py:147] round_filter input=112 output=176\n",
      "I1124 04:48:54.759765 140200370878336 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I1124 04:48:55.627802 140200370878336 efficientnet_model.py:147] round_filter input=192 output=304\n",
      "I1124 04:48:55.628010 140200370878336 efficientnet_model.py:147] round_filter input=320 output=512\n",
      "I1124 04:48:55.907521 140200370878336 efficientnet_model.py:147] round_filter input=1280 output=2048\n",
      "I1124 04:48:55.949782 140200370878336 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 04:48:56.055235 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
      "I1124 04:48:56.055433 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I1124 04:48:56.055547 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 8\n",
      "I1124 04:48:56.057535 140200370878336 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I1124 04:48:56.081801 140200370878336 efficientnet_model.py:147] round_filter input=32 output=56\n",
      "I1124 04:48:56.081984 140200370878336 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1124 04:48:56.311104 140200370878336 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1124 04:48:56.311329 140200370878336 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1124 04:48:56.890065 140200370878336 efficientnet_model.py:147] round_filter input=24 output=40\n",
      "I1124 04:48:56.890282 140200370878336 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I1124 04:48:57.486266 140200370878336 efficientnet_model.py:147] round_filter input=40 output=72\n",
      "I1124 04:48:57.486515 140200370878336 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I1124 04:48:58.257021 140200370878336 efficientnet_model.py:147] round_filter input=80 output=144\n",
      "I1124 04:48:58.257231 140200370878336 efficientnet_model.py:147] round_filter input=112 output=200\n",
      "I1124 04:48:59.256053 140200370878336 efficientnet_model.py:147] round_filter input=112 output=200\n",
      "I1124 04:48:59.256251 140200370878336 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I1124 04:49:00.307698 140200370878336 efficientnet_model.py:147] round_filter input=192 output=344\n",
      "I1124 04:49:00.307931 140200370878336 efficientnet_model.py:147] round_filter input=320 output=576\n",
      "I1124 04:49:00.595292 140200370878336 efficientnet_model.py:147] round_filter input=1280 output=2304\n",
      "I1124 04:49:00.630211 140200370878336 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1124 04:49:00.753061 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
      "I1124 04:49:00.753271 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
      "I1124 04:49:00.753397 140200370878336 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 8\n",
      "I1124 04:49:00.755403 140200370878336 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I1124 04:49:00.774827 140200370878336 efficientnet_model.py:147] round_filter input=32 output=64\n",
      "I1124 04:49:00.774979 140200370878336 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1124 04:49:01.082090 140200370878336 efficientnet_model.py:147] round_filter input=16 output=32\n",
      "I1124 04:49:01.082308 140200370878336 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I1124 04:49:01.755517 140200370878336 efficientnet_model.py:147] round_filter input=24 output=48\n",
      "I1124 04:49:01.755748 140200370878336 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I1124 04:49:02.425044 140200370878336 efficientnet_model.py:147] round_filter input=40 output=80\n",
      "I1124 04:49:02.425280 140200370878336 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I1124 04:49:03.372860 140200370878336 efficientnet_model.py:147] round_filter input=80 output=160\n",
      "I1124 04:49:03.373099 140200370878336 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I1124 04:49:04.312606 140200370878336 efficientnet_model.py:147] round_filter input=112 output=224\n",
      "I1124 04:49:04.312920 140200370878336 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I1124 04:49:05.789267 140200370878336 efficientnet_model.py:147] round_filter input=192 output=384\n",
      "I1124 04:49:05.789509 140200370878336 efficientnet_model.py:147] round_filter input=320 output=640\n",
      "I1124 04:49:06.167812 140200370878336 efficientnet_model.py:147] round_filter input=1280 output=2560\n",
      "I1124 04:49:06.211844 140200370878336 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 27.06s\n",
      "I1124 04:49:06.360397 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 27.06s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "I1124 04:49:06.367859 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "I1124 04:49:06.369791 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "I1124 04:49:06.370443 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "I1124 04:49:06.372341 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTF2Test.test_session\n",
      "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "I1124 04:49:06.377148 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "I1124 04:49:06.378277 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "I1124 04:49:06.380637 140200370878336 test_util.py:2189] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 24 tests in 32.833s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "# From within TensorFlow/models/research/\n",
    "!python object_detection/builders/model_builder_tf2_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc382MCGBuDl"
   },
   "source": [
    "### Preparing the workplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TxPzE3UKHlt",
    "outputId": "59eab01b-b9fb-47d3-db4f-578705260015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CR_TDc5KXtd",
    "outputId": "d7dc9fe5-c74b-4388-c3ea-8b2f9aa3566c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'55416691_Oris Pratama_acc skripsi.pdf.ribd'\n",
      "'55416691_Oris Pratama_Agenda bimbingan.pdf.ribd'\n",
      "'Bahasa Indonesia dan Generasi Kini.gdoc'\n",
      "'Colab Notebooks'\n",
      "'Dokumentasi for OCR.gdoc'\n",
      "'File PI'\n",
      "'Form Komitmen dan Form Pertanggungjawaban PRODTS2021. [Peserta].gdoc.gdoc'\n",
      " img027.pdf\n",
      " img028.jpeg\n",
      " img028.pdf\n",
      " LPR.gslides.gslides\n",
      "'my file'\n",
      "'Oris Pratama – UG – TI – Javascript-Android Java.pdf.ribd'\n",
      " P_20201218_131950.jpg.ribd\n",
      " report.csv\n",
      " report.gsheet\n",
      "'Research LPR.gdoc.gdoc'\n",
      " Skripsi\n",
      "'Skripsi SK Teknik Informatika 2020_Lintang_edited.xlsx.ribd'\n",
      "'Standup Special Pandji'\n",
      " tensor\n",
      "'Untitled document.gdoc'\n",
      " yolov3\n",
      " yolov4\n"
     ]
    }
   ],
   "source": [
    "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
    "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
    "!ls /mydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMXzIKmvIuqB",
    "outputId": "80397f69-b490-4312-997a-684c0f5aa686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'tf_od_tutorial'...\n",
      "remote: Enumerating objects: 3954, done.\u001b[K\n",
      "remote: Total 3954 (delta 0), reused 0 (delta 0), pack-reused 3954\u001b[K\n",
      "Receiving objects: 100% (3954/3954), 281.91 MiB | 33.85 MiB/s, done.\n",
      "Resolving deltas: 100% (1942/1942), done.\n",
      "Checking out files: 100% (3818/3818), done.\n",
      "/content/tf_od_tutorial\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "repo_url = 'https://github.com/orizzz/tf_od_tutorial'\n",
    "%cd /content\n",
    "\n",
    "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
    "\n",
    "!git clone {repo_url}\n",
    "%cd {repo_dir_path}\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNxbelQmJDVV",
    "outputId": "ed8ec5f3-555a-4b88-d0a1-ccdb3efb8f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/                       README.md\n",
      "\u001b[01;34mdeploy\u001b[0m/                     requirements.txt\n",
      "generate_tfrecord.py        resize_images.py\n",
      "label_map.pbtxt             spn_tensorflow_training.ipynb\n",
      "LICENSE                     tensorflow_object_detection_training_colab.ipynb\n",
      "local_inference_test.ipynb  \u001b[01;34mtest\u001b[0m/\n",
      "local_inference_test.py     xml_to_csv.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8qJbV1uauAG"
   },
   "source": [
    "### Preparing the necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2bohyWtB6Es",
    "outputId": "bff44493-db33-417d-8bed-db65d7db0c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p /mydrive/tensor/training_demo\n",
    "%cd /mydrive/tensor/training_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8YaBALaCITV",
    "outputId": "9b6ee546-a2db-4cf8-c933-84ccb5bce01c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pesLUuIzCl4X",
    "outputId": "769322fd-9aa6-46e5-c199-87611dafb8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/  \u001b[01;34mexported_models\u001b[0m/  \u001b[01;34mimages\u001b[0m/  \u001b[01;34mpre-trained-models\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p annotations\n",
    "%mkdir -p exported_models\n",
    "%mkdir -p images\n",
    "%mkdir -p pre-trained-models\n",
    "%ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aQSRxusJg9t"
   },
   "outputs": [],
   "source": [
    "%cp -r {repo_dir_path}/data/images /content/training_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u5NF3LoaZs5"
   },
   "outputs": [],
   "source": [
    "%cd /content/training_demo/images\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aM7hbvyua_lV"
   },
   "outputs": [],
   "source": [
    "%cd /content/training_demo/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3wQwqGoaw5h",
    "outputId": "5285b372-0979-4859-bf0a-26b35c5ad303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/      generate_tfrecord.py  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/  \u001b[01;34mimages\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cp -r {repo_dir_path}/generate_tfrecord.py /mydrive/tensor/training_demo\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A83mzmtlDi5z",
    "outputId": "7f043653-5167-436d-f158-993fe07d1ad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/      generate_tfrecord.py  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/  \u001b[01;34mimages\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cp -r {repo_dir_path}/label_map.pbtxt /mydrive/tensor/training_demo/annotations\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gL4BBRoZuWr8",
    "outputId": "736fbd0a-1352-4f2f-cbeb-34013980d5cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo/pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd /mydrive/tensor/training_demo/pre-trained-models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MItGLVY3uWu8",
    "outputId": "c5821842-e5d7-42ef-da17-11805e2fdc6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-24 06:44:53--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.157.128, 2404:6800:4008:c13::80\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.157.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 46042990 (44M) [application/x-tar]\n",
      "Saving to: ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz’\n",
      "\n",
      "ssd_mobilenet_v2_32 100%[===================>]  43.91M  19.4MB/s    in 2.3s    \n",
      "\n",
      "2021-11-24 06:44:57 (19.4 MB/s) - ‘ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz’ saved [46042990/46042990]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzlPcDPLuWye",
    "outputId": "62af3c61-af9f-4c3a-dcb1-b0c28607bb8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/checkpoint\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ckpt-0.index\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/saved_model.pb\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
      "ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Vf49zVcAtNJB",
    "outputId": "23514e80-8763-4dca-a9e5-4f43afe31c97"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/gdrive/My Drive/tensor/training_demo/pre-trained-models'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFA0L4rmyGae",
    "outputId": "a9d9c336-3d6a-4526-85cd-932873d1e124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "source": [
    "cd /mydrive/tensor/training_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxveA1zpALrB"
   },
   "outputs": [],
   "source": [
    "%cp -r {repo_dir_path}/data/models /mydrive/tensor/training_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pxst7lPT0Sby",
    "outputId": "f07617ef-28cd-4994-fabc-31c451494b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                          \u001b[01;34mimages\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/                                      \u001b[01;34mmodels\u001b[0m/\n",
      "faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "generate_tfrecord.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hU2lVZfzyuar",
    "outputId": "2f8d192e-f828-4469-c695-e2acb46cccaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: /mydrive/tensor/training_demo/annotations/train.record\n",
      "Successfully created the TFRecord file: /mydrive/tensor/training_demo/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "# Create train data:\n",
    "!python generate_tfrecord.py -x {repo_dir_path}/data/images/train -l /mydrive/tensor/training_demo/annotations/label_map.pbtxt -o /mydrive/tensor/training_demo/annotations/train.record\n",
    "\n",
    "# Create test data:\n",
    "!python generate_tfrecord.py -x {repo_dir_path}/data/images/test -l /mydrive/tensor/training_demo/annotations/label_map.pbtxt -o /mydrive/tensor/training_demo/annotations/test.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "MfHABHxi56kM",
    "outputId": "10c0a8e4-80b6-4e44-a567-6756bb5ca777"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/gdrive/My Drive/tensor/training_demo'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQO8P8xLy6bS",
    "outputId": "768e5011-d61d-455f-9b9f-e3cb278d0f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                          \u001b[01;34mimages\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/                                      \u001b[01;34mmodels\u001b[0m/\n",
      "faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz  \u001b[01;34mpre-trained-models\u001b[0m/\n",
      "generate_tfrecord.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-XrMHcKOVWD0",
    "outputId": "f30cb38b-d622-498d-aedc-43b886338931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                          \u001b[01;34mimages\u001b[0m/\n",
      "\u001b[01;34mexported_models\u001b[0m/                                      model_main_tf2.py\n",
      "faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz  \u001b[01;34mmodels\u001b[0m/\n",
      "generate_tfrecord.py                                  \u001b[01;34mpre-trained-models\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cp -r /content/models/research/object_detection/model_main_tf2.py /mydrive/tensor/training_demo\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JQFLVmCa4Yo"
   },
   "source": [
    "### Start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uO_IHATIEqV1",
    "outputId": "7a85ed62-fc90-4d9d-a7d2-897af851b8d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/tensor/training_demo\n"
     ]
    }
   ],
   "source": [
    "cd /mydrive/tensor/training_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNVwlSCq9pr1",
    "outputId": "209122fb-9ff2-439f-ab03-2f45a1419851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-24 04:50:22.020668: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "I1124 04:50:22.026642 140015312131968 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\n",
      "I1124 04:50:22.551321 140015312131968 config_util.py:552] Maybe overwriting train_steps: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1124 04:50:22.551607 140015312131968 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W1124 04:50:22.595784 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/mydrive/tensor/training_demo/annotations/train.record']\n",
      "I1124 04:50:22.741549 140015312131968 dataset_builder.py:163] Reading unweighted datasets: ['/mydrive/tensor/training_demo/annotations/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/mydrive/tensor/training_demo/annotations/train.record']\n",
      "I1124 04:50:22.742175 140015312131968 dataset_builder.py:80] Reading record datasets for input file: ['/mydrive/tensor/training_demo/annotations/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1124 04:50:22.742341 140015312131968 dataset_builder.py:81] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1124 04:50:22.742489 140015312131968 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "W1124 04:50:22.746255 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1124 04:50:22.769541 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1124 04:50:31.836089 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1124 04:50:35.647020 140015312131968 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1124 04:50:55.902511 140011229746944 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "W1124 04:51:09.800204 140011229746944 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W1124 04:51:17.665125 140011229746944 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2021-11-24 04:51:47.954507: E tensorflow/stream_executor/cuda/cuda_dnn.cc:362] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2021-11-24 04:51:47.958479: E tensorflow/stream_executor/cuda/cuda_dnn.cc:362] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "Traceback (most recent call last):\n",
      "  File \"model_main_tf2.py\", line 115, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"model_main_tf2.py\", line 112, in main\n",
      "    record_summaries=FLAGS.record_summaries)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 603, in train_loop\n",
      "    train_input, unpad_groundtruth_tensors)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 394, in load_fine_tune_checkpoint\n",
      "    _ensure_model_is_built(model, input_dataset, unpad_groundtruth_tensors)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 176, in _ensure_model_is_built\n",
      "    labels,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1286, in run\n",
      "    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 2849, in call_for_each_replica\n",
      "    return self._call_for_each_replica(fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 671, in _call_for_each_replica\n",
      "    self._container_strategy(), fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_run.py\", line 86, in call_for_each_replica\n",
      "    return wrapped(args, kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 950, in _call\n",
      "    return self._stateless_fn(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3040, in __call__\n",
      "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1964, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 596, in call\n",
      "    ctx=ctx)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\n",
      "  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
      "\t [[node model/conv1_conv/Conv2D (defined at /local/lib/python3.7/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1346) ]]\n",
      "\t [[Loss/RPNLoss/BalancedPositiveNegativeSampler_1/Cast_8/_588]]\n",
      "  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
      "\t [[node model/conv1_conv/Conv2D (defined at /local/lib/python3.7/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1346) ]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference__dummy_computation_fn_44910]\n",
      "\n",
      "Errors may have originated from an input operation.\n",
      "Input Source operations connected to node model/conv1_conv/Conv2D:\n",
      " model/lambda/Pad (defined at /local/lib/python3.7/dist-packages/object_detection/models/keras_models/resnet_v1.py:51)\n",
      "\n",
      "Input Source operations connected to node model/conv1_conv/Conv2D:\n",
      " model/lambda/Pad (defined at /local/lib/python3.7/dist-packages/object_detection/models/keras_models/resnet_v1.py:51)\n",
      "\n",
      "Function call stack:\n",
      "_dummy_computation_fn -> _dummy_computation_fn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python model_main_tf2.py --model_dir=/mydrive/tensor/training_demo/models/spn_ssd_mobilenet_v2_320x320 --pipeline_config_path=/mydrive/tensor/training_demo/models/spn_ssd_mobilenet_v2_320x320/pipeline.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vT8TkYh-OISw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TlRqjT4AFOhv",
    "outputId": "a77528bd-f60a-4846-cf1a-f2b7eb8d3513"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/training_demo'"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeI2URnR9zhw",
    "outputId": "2defe90f-c72b-49e4-a7c3-e8d6cdf145b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'exporter_main_v2.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python exporter_main_v2.py --input_type image_tensor --pipeline_config_path /content/training_demo/models/spn_ssd_mobilenet_v2_320x320/pipeline.config --trained_checkpoint_dir /content/training_demo/models/spn_ssd_mobilenet_v2_320x320 --output_directory /content/training_demo/exported_models/spn_detection_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIIKwpxtGAhK"
   },
   "source": [
    "Inferencing My Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spZJ4ms3FqRT"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Object Detection (On Image) From TF2 Saved Model\n",
    "=====================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import argparse\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# PROVIDE PATH TO IMAGE DIRECTORY\n",
    "IMAGE_PATHS = '/content/training_demo/images/train/image1.jpg'\n",
    "\n",
    "\n",
    "# PROVIDE PATH TO MODEL DIRECTORY\n",
    "PATH_TO_MODEL_DIR = '/content/training_demo/exported_models/my_model'\n",
    "\n",
    "# PROVIDE PATH TO LABEL MAP\n",
    "PATH_TO_LABELS = '/content/training_demo/annotations/label_map.pbtxt'\n",
    "\n",
    "# PROVIDE THE MINIMUM CONFIDENCE THRESHOLD\n",
    "MIN_CONF_THRESH = float(0.60)\n",
    "\n",
    "# LOAD THE MODEL\n",
    "\n",
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))\n",
    "\n",
    "# LOAD LABEL MAP DATA FOR PLOTTING\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Running inference for {}... '.format(IMAGE_PATHS), end='')\n",
    "\n",
    "image = cv2.imread(IMAGE_PATHS)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_expanded = np.expand_dims(image_rgb, axis=0)\n",
    "\n",
    "# The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "input_tensor = tf.convert_to_tensor(image)\n",
    "# The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "# input_tensor = np.expand_dims(image_np, 0)\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "# All outputs are batches tensors.\n",
    "# Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "# We're only interested in the first num_detections.\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "               for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "image_with_detections = image.copy()\n",
    "\n",
    "# SET MIN_SCORE_THRESH BASED ON YOU MINIMUM THRESHOLD FOR DETECTIONS\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_with_detections,\n",
    "      detections['detection_boxes'],\n",
    "      detections['detection_classes'],\n",
    "      detections['detection_scores'],\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=0.5,\n",
    "      agnostic_mode=False)\n",
    "\n",
    "print('Done')\n",
    "# DISPLAYS OUTPUT IMAGE\n",
    "cv2_imshow(image_with_detections)\n",
    "# CLOSES WINDOW ONCE KEY IS PRESSED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0KheEfPGYhO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of spn tensorflow training di akun spn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
